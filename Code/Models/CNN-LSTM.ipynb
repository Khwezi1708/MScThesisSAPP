{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f136a04",
   "metadata": {},
   "source": [
    "This file will run an LSTM model to predict the electricity prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b03c0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import plot_comparison, evaluate_lstm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26a92140",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Data/zra_sgp_dam.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0032cb70",
   "metadata": {},
   "source": [
    "# Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e6a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cyclic(df, col, max_val):\n",
    "    \"\"\"\"\n",
    "    Time features like Hour, Month, day_of_week are cyclical, not linear. \n",
    "    Without encoding them properly, the model will misunderstand their relationships.\n",
    "    \"\"\"\n",
    "    df[col + '_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "    df[col + '_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "    return df\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Cyclic encode time features\n",
    "    df = encode_cyclic(df, 'Hour', 24)\n",
    "    df = encode_cyclic(df, 'Month', 12)\n",
    "    df = encode_cyclic(df, 'day_of_week', 7)\n",
    "    \n",
    "    # Drop unused or problematic columns\n",
    "    df = df.drop(columns=['Hour', 'Month', 'day_of_week'])  # Keep cyclic versions instead\n",
    "    \n",
    "    # Fill/clean if needed\n",
    "    df = df.fillna(method='ffill').dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96285121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequence for LSTM\n",
    "def create_sequences(data, target_col, n_steps=48, forecast_horizon=24):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps - forecast_horizon):\n",
    "        seq_x = data.iloc[i:i+n_steps].drop(columns=[target_col]).values\n",
    "        seq_y = data.iloc[i+n_steps:i+n_steps+forecast_horizon][target_col].values\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e10133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7508771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Lstm model\n",
    "class LSTMForecast(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2, output_size=24):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0.0, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x) # Get all outputs\n",
    "        out = output[:, -1, :] # Use output from the last timestep\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e8b35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.log(torch.cosh(y_pred - y_true + 1e-12))  # added epsilon to prevent log(0)\n",
    "        return torch.mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "def train_model(model, dataloader, val_dataloader=None, epochs=10, lr=1e-3, patience=10, min_delta=1e-4):\n",
    "    device = next(model.parameters()).device\n",
    "    criterion = LogCoshLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = copy.deepcopy(model.state_dict())\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        loss_history.append(avg_loss)\n",
    "\n",
    "        if val_dataloader:\n",
    "            model.eval()\n",
    "            val_total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_dataloader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    pred = model(xb)\n",
    "                    loss = criterion(pred, yb)\n",
    "                    val_total_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = val_total_loss / len(val_dataloader)\n",
    "            val_loss_history.append(avg_val_loss)\n",
    "\n",
    "            if best_val_loss - avg_val_loss > min_delta:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"⏹️ Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if val_dataloader:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return loss_history, val_loss_history if val_dataloader else loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "556c9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_history):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(loss_history, label='Training loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f56ff6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_importance(importances, feature_names=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    names = list(feature_names) if feature_names is not None else [f\"Feature {i}\" for i in range(len(importances))]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(importances)), importances[indices])\n",
    "    plt.xticks(range(len(importances)), [names[i] for i in indices], rotation=45, ha='right')\n",
    "    plt.title(\"SHAP Feature Importance (Averaged)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4f5eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_datetime(model, df, timestamp, n_steps=48, target_col='Price (USD/MWh)', scaler_y=None):\n",
    "    \"\"\"\n",
    "    Predict 24-hour prices starting from a given timestamp.\n",
    "\n",
    "    Args:\n",
    "        model: Trained LSTMForecast model.\n",
    "        df: Preprocessed + scaled DataFrame (with DateTime index).\n",
    "        timestamp: Datetime string or pd.Timestamp (e.g. '2023-01-01 00:00').\n",
    "        n_steps: Number of past hours to use (default = 48).\n",
    "        target_col: Name of target column.\n",
    "        scaler_y: Scaler used for the target column.\n",
    "\n",
    "    Returns:\n",
    "        List of (datetime, predicted_price) tuples.\n",
    "    \"\"\"\n",
    "    if isinstance(timestamp, str):\n",
    "        timestamp = pd.Timestamp(timestamp)\n",
    "        \n",
    "    # Check if enough history is available\n",
    "    start_idx = df.index.get_loc(timestamp)\n",
    "    if start_idx < n_steps:\n",
    "        raise ValueError(\"Not enough history before this timestamp.\")\n",
    "\n",
    "    # Build the input sequence (excluding target columns)\n",
    "    seq_df = df.iloc[start_idx - n_steps:start_idx].drop(columns=[target_col, 'target_scaled'])\n",
    "    seq_input = seq_df.values  # shape: (n_steps, num_features)\n",
    "\n",
    "    # Predict the future prices\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(seq_input[np.newaxis, :, :], dtype=torch.float32)\n",
    "        y_pred = model(x).squeeze().numpy()  # shape: (forecast_horizon, )\n",
    "\n",
    "    # Reverse scaling (if provided)\n",
    "    if scaler_y is not None:\n",
    "        y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1))  # Inverse transform predictions\n",
    "        y_pred_original_flat = y_pred_original.flatten()  # Flatten to 1D\n",
    "    else:\n",
    "        y_pred_original_flat = y_pred  # If no scaler, use raw predictions\n",
    "\n",
    "    # Build future timestamps\n",
    "    future_times = [timestamp + pd.Timedelta(hours=i) for i in range(24)]\n",
    "    \n",
    "    return list(zip(future_times, y_pred_original_flat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aed1bc",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7ca40c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t_/0kjgy0zn0k10thg9kn806w140000gn/T/ipykernel_2693/4031471906.py:22: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='ffill').dropna()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Preprocess (make sure 'Date' becomes index)\n",
    "df_clean = preprocess(df)\n",
    "\n",
    "# Define target\n",
    "target_col = 'Price (USD/MWh)'\n",
    "features = df_clean.drop(columns=[target_col])\n",
    "target = df_clean[target_col]\n",
    "\n",
    "# Columns by type\n",
    "minmax_cols = ['Tati- normalised output', 'E_Grid (Mw)', 'Revenues (USD)', \n",
    "               'Flow_chavuma', 'Level_kariba', 'Flow_nana']\n",
    "standard_cols = ['Volatility_1 Day', 'Volatility_3 Days', 'Volatility_7 Days', 'Volatility_30 Days',\n",
    "                 'roc_49h', 'momentum_49h']\n",
    "no_scaling_cols = ['Hour_sin', 'Hour_cos', 'Month_sin', 'Month_cos',\n",
    "                   'day_of_week_sin', 'day_of_week_cos']\n",
    "\n",
    "# Initialize scalers\n",
    "minmax_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Copy clean DataFrame\n",
    "df_scaled = df_clean.copy()\n",
    "\n",
    "# Apply scalers to appropriate columns\n",
    "df_scaled[minmax_cols] = minmax_scaler.fit_transform(df_clean[minmax_cols])\n",
    "df_scaled[standard_cols] = standard_scaler.fit_transform(df_clean[standard_cols])\n",
    "\n",
    "# Target scaling (fit only on the column, keep shape)\n",
    "df_scaled[\"target_scaled\"] = scaler_y.fit_transform(df_clean[[target_col]])\n",
    "\n",
    "# Optionally retain unscaled target for reference\n",
    "df_scaled[target_col] = target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e038617",
   "metadata": {},
   "source": [
    "# CNN\n",
    "CNN-based feature extractor before creating sequences for your time series model.\n",
    "Use a 1D CNN to learn temporal features from the multivariate time series, and combine those with your original engineered features (like Fourier features, volatility, etc.) to build an enriched LSTM input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "665894bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels=16, kernel_size=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, out_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels, sequence_length)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        return x  # shape: (batch, out_channels, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a0b6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_cnn_over_time(df_features, window=48, stride=1, device='cpu'):\n",
    "    \"\"\"\n",
    "    Apply CNN over rolling time windows of multivariate input.\n",
    "    Returns a DataFrame with CNN features per time step (centered).\n",
    "    \"\"\"\n",
    "    feature_cols = df_features.columns\n",
    "    num_features = len(feature_cols)\n",
    "    num_windows = (len(df_features) - window) // stride + 1\n",
    "\n",
    "    # Prepare input tensor\n",
    "    X_cnn = []\n",
    "    for i in range(0, len(df_features) - window + 1, stride):\n",
    "        window_data = df_features.iloc[i:i+window].values.T  # shape: (features, window)\n",
    "        X_cnn.append(window_data)\n",
    "\n",
    "    X_cnn = torch.tensor(np.stack(X_cnn), dtype=torch.float32).to(device)  # shape: (batch, features, window)\n",
    "\n",
    "    # CNN model\n",
    "    cnn = CNNFeatureExtractor(input_channels=num_features).to(device)\n",
    "    cnn.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features_out = cnn(X_cnn)  # shape: (batch, out_channels, window)\n",
    "    \n",
    "    # Collapse time dimension (e.g., take mean over time)\n",
    "    pooled = features_out.mean(dim=2).cpu().numpy()  # shape: (batch, out_channels)\n",
    "\n",
    "    # Align timestamps to center of each window\n",
    "    start_index = window // 2\n",
    "    end_index = start_index + len(pooled)\n",
    "    timestamps = df_features.index[start_index:end_index]\n",
    "\n",
    "    return pd.DataFrame(pooled, index=timestamps, columns=[f'cnn_feat_{i}' for i in range(pooled.shape[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a854f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_features_df = apply_cnn_over_time(\n",
    "    df_scaled[minmax_cols + standard_cols + no_scaling_cols],  # all features\n",
    "    window=48,\n",
    "    stride=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3338055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align target with features\n",
    "cnn_features_df[\"target_scaled\"] = df_scaled[\"target_scaled\"].loc[cnn_features_df.index]\n",
    "df_lstm_input = cnn_features_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d876fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 24 * 7 # 30 days of history\n",
    "# Create sequences\n",
    "X, y = create_sequences(df_scaled.drop(columns=['Price (USD/MWh)']), target_col='target_scaled', n_steps=lookback, forecast_horizon=24)\n",
    "\n",
    "# Train-validation-test split (70-15-15 split)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Create datasets\n",
    "train_ds = PriceDataset(X_train, y_train)\n",
    "val_ds = PriceDataset(X_val, y_val)\n",
    "test_ds = PriceDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=False, num_workers=1)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=1)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d531474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import uniform\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming LSTMForecast and train_ds are defined elsewhere\n",
    "# Also assuming val_loader is defined somewhere\n",
    "\n",
    "input_size = X.shape[2]  # Number of features (should match the input size of the LSTM)\n",
    "\n",
    "class WOA:\n",
    "    def __init__(self, n_whales, max_iter, bounds):\n",
    "        self.n_whales = n_whales\n",
    "        self.max_iter = max_iter\n",
    "        self.bounds = bounds\n",
    "        self.population = self.init_population()\n",
    "        self.best_whale = None\n",
    "        self.best_fitness = float('inf')\n",
    "\n",
    "    def init_population(self):\n",
    "        \"\"\"Initialize whale population randomly within the specified bounds.\"\"\"\n",
    "        population = []\n",
    "        for _ in range(self.n_whales):\n",
    "            whale = [uniform(*self.bounds[param]) for param in self.bounds]\n",
    "            population.append(whale)\n",
    "        return population\n",
    "\n",
    "    def smape(self, y_true, y_pred):\n",
    "        \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "        smape = np.mean(np.abs(y_pred - y_true) / denominator) * 100\n",
    "        return smape\n",
    "    \n",
    "    def evaluate_model(self, model, val_loader, device):\n",
    "        \"\"\"Evaluate the model on the validation set.\"\"\"\n",
    "        model.to(device)  # Ensure model is on the correct device (GPU or CPU)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        with torch.no_grad():  # No gradient calculation needed during evaluation\n",
    "            for xb, yb in val_loader:  # Iterate over validation data\n",
    "                xb, yb = xb.to(device), yb.to(device)  # Move data to the correct device\n",
    "                output = model(xb)  # Model's predictions\n",
    "\n",
    "                y_true.append(yb.cpu().numpy())  # Store actual values\n",
    "                y_pred.append(output.cpu().numpy())  # Store predicted values\n",
    "\n",
    "        # Flatten the lists into 1D arrays for easy SMAPE calculation\n",
    "        y_true = np.concatenate(y_true, axis=0)\n",
    "        y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "        return y_true, y_pred\n",
    "\n",
    "    def fitness(self, whale):\n",
    "        \"\"\"Calculate fitness (SMAPE) of the current whale configuration.\"\"\"\n",
    "        hidden_size, num_layers, dropout, lr = whale\n",
    "\n",
    "        N_subset = 8760 - lookback\n",
    "        subset_indices = list(range(N_subset))\n",
    "        subset_train_ds = torch.utils.data.Subset(train_ds, subset_indices)\n",
    "        subset_train_loader = DataLoader(subset_train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        model = WT_LSTMForecast(\n",
    "            input_size=input_size,\n",
    "            hidden_size=int(hidden_size),\n",
    "            num_layers=int(num_layers),\n",
    "            dropout=dropout,\n",
    "            output_dim=4\n",
    "        ).to(device)  # Move the model to the correct device\n",
    "\n",
    "        # Get actuals and predictions from validation\n",
    "        y_true, y_pred = self.evaluate_model(model, val_loader, device)\n",
    "\n",
    "        return self.smape(y_true, y_pred)\n",
    "\n",
    "\n",
    "    def update_position(self, whale, a, best_whale):\n",
    "        \"\"\"Update the position of the whale based on the hunting behavior.\"\"\"\n",
    "        A = 2 * a * np.random.rand() - a  # Randomization for exploration\n",
    "        C = 2 * np.random.rand()  # Another randomization factor for exploration\n",
    "        p = np.random.rand()  # Probability for exploitation or exploration\n",
    "\n",
    "        if p < 0.5:\n",
    "            if np.abs(A) >= 1:\n",
    "                # Exploration: Random movement\n",
    "                rand_whale = self.population[np.random.randint(self.n_whales)]\n",
    "                new_whale = np.array(rand_whale) - A * np.abs(C * np.array(rand_whale) - np.array(whale))\n",
    "            else:\n",
    "                # Exploitation: Move towards best whale\n",
    "                new_whale = np.array(best_whale) - A * np.abs(C * np.array(best_whale) - np.array(whale))\n",
    "        else:\n",
    "            distance_best = np.abs(np.array(best_whale) - np.array(whale))\n",
    "            new_whale = distance_best * np.exp(a * distance_best) * np.cos(2 * np.pi * np.random.rand())\n",
    "\n",
    "        # Bound check: Ensure the new whale is within bounds\n",
    "        new_whale = np.clip(new_whale, [self.bounds[param][0] for param in self.bounds],\n",
    "                            [self.bounds[param][1] for param in self.bounds])\n",
    "        \n",
    "        return new_whale\n",
    "\n",
    "    def optimize(self, smape_threshold=10.0):\n",
    "        \"\"\"Run the Whale Optimization Algorithm.\"\"\"\n",
    "        for t in tqdm(range(self.max_iter), desc=\"WOA Iterations\"):\n",
    "            a = 2 - t * (2 / self.max_iter)\n",
    "\n",
    "            for i in tqdm(range(self.n_whales), desc=f\"Whales (Iteration {t+1})\", leave=False):\n",
    "                whale = self.population[i]\n",
    "                hidden_size, num_layers, dropout, lr = whale\n",
    "\n",
    "                # print(f\"Iteration {t+1}, Whale {i+1}: hidden_size={int(hidden_size)}, \"\n",
    "                #       f\"num_layers={int(num_layers)}, dropout={dropout:.3f}, lr={lr:.6f}\")\n",
    "\n",
    "                fitness_val = self.fitness(whale)\n",
    "\n",
    "                if fitness_val < self.best_fitness:\n",
    "                    self.best_fitness = fitness_val\n",
    "                    self.best_whale = whale\n",
    "\n",
    "                    # Early stopping if precision threshold met\n",
    "                    if self.best_fitness <= smape_threshold:\n",
    "                        # print(f\"\\nStopping early at iteration {t+1} with SMAPE {self.best_fitness:.2f}\")\n",
    "                        return self.best_whale, self.best_fitness\n",
    "\n",
    "                self.population[i] = self.update_position(whale, a, self.best_whale)\n",
    "\n",
    "        return self.best_whale, self.best_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb30de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WOA Iterations: 100%|██████████| 50/50 [03:21<00:00,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best configuration from WOA:\n",
      "Best Params: [3.2e+01 1.0e+00 1.0e-01 5.0e-04]\n",
      "Best Validation Loss: 135.89743041992188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the bounds for the hyperparameters\n",
    "bounds = {\n",
    "    'hidden_size': (32, 256),  # Increased hidden size range to 256\n",
    "    'num_layers': (1, 4),      # Increased num_layers range to 4\n",
    "    'dropout': (0.1, 0.5),     # Increased dropout range to 0.5\n",
    "    'lr': (5e-4, 1e-2)         # Increased learning rate range to 1e-2\n",
    "}\n",
    "\n",
    "# Create the WOA optimizer with the updated bounds\n",
    "woa = WOA(n_whales=10, max_iter=50, bounds=bounds)\n",
    "# Run optimization\n",
    "best_params, best_val_loss = woa.optimize()\n",
    "\n",
    "# Output the best found parameters and corresponding validation loss\n",
    "print(\"\\nBest configuration from WOA:\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4341b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = [3.2e+01, 1.0e+00, 1.0e-01, 5.0e-04]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba8d58eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'PriceDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 2822) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m wait([\u001b[38;5;28mself\u001b[39m], timeout)\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/connection.py:1136\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m     ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 2822) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model with timing\u001b[39;00m\n\u001b[1;32m     16\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 18\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     19\u001b[0m     model,\n\u001b[1;32m     20\u001b[0m     dataloader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m     21\u001b[0m     val_dataloader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[1;32m     22\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     23\u001b[0m     lr\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;241m3\u001b[39m],\n\u001b[1;32m     24\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,       \u001b[38;5;66;03m# stop if no val improvement\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m     \u001b[38;5;66;03m# must improve by at least this much\u001b[39;00m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     29\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, val_dataloader, epochs, lr, patience, min_delta)\u001b[0m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     19\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     21\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(device), yb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(xb)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1420\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1420\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1422\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1264\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1263\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1266\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 2822) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "\"Train model with best params from WOA\"\n",
    "import time  # Add this at the top of your script if not already imported\n",
    "# Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = X.shape[2]  # Number of features (should match the input size of the LSTM)\n",
    "\n",
    "model = LSTMForecast(\n",
    "    input_size=input_size,\n",
    "    hidden_size=int(256),  # Ensure integer value for hidden_size\n",
    "    num_layers=int(2),   # Ensure integer value for num_layers\n",
    "    dropout=best_params[2]\n",
    ").to(device)\n",
    "\n",
    "# Train the model with timing\n",
    "start_time = time.time()\n",
    "\n",
    "train_losses, val_losses = train_model(\n",
    "    model,\n",
    "    dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    epochs=100,\n",
    "    lr=best_params[3],\n",
    "    patience=15,       # stop if no val improvement\n",
    "    min_delta=1e-4     # must improve by at least this much\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTraining completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "plot_loss(train_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af78dd",
   "metadata": {},
   "source": [
    "# Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch model to eval mode\n",
    "model.eval()\n",
    "\n",
    "def get_predictions(model, dataloader):\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in dataloader:\n",
    "            xb = xb.to(device)\n",
    "            preds = model(xb)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "    return np.concatenate(all_preds, axis=0)\n",
    "\n",
    "# Get predictions from validation and test loaders\n",
    "val_pred_np = get_predictions(model, val_loader)  # <-- Now this is defined\n",
    "test_pred_np = get_predictions(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e178d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_from_coeffs(y_pred, wavelet='db1'):\n",
    "    \"\"\"\n",
    "    Reconstruct time series from predicted DWT coefficients.\n",
    "    y_pred: numpy array of shape (samples, forecast_horizon, 4)\n",
    "    Returns:\n",
    "        reconstructed: numpy array of shape (samples, forecast_horizon)\n",
    "    \"\"\"\n",
    "    reconstructed = []\n",
    "    for sample in y_pred:\n",
    "        # sample: (forecast_horizon, 4)\n",
    "        sample_recon = []\n",
    "        for t in range(sample.shape[0]):\n",
    "            cA3, cD3, cD2, cD1 = sample[t]\n",
    "\n",
    "            # Use pywt.waverec to reconstruct from multi-level coeffs\n",
    "            coeffs = [np.array([cA3]), np.array([cD3]), np.array([cD2]), np.array([cD1])]\n",
    "            signal = pywt.waverec(coeffs, wavelet)\n",
    "\n",
    "            # Take only the first value (or mean) to align with time step t\n",
    "            sample_recon.append(signal[0])  # or signal.mean(), etc.\n",
    "\n",
    "        reconstructed.append(sample_recon)\n",
    "\n",
    "    return np.array(reconstructed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c242339",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = reconstruct_from_coeffs(val_pred_np)\n",
    "reconstructed = scaler_y.inverse_transform(reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3e7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1203, 24, 4)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_val = reconstruct_from_coeffs(y_val)\n",
    "true_test = reconstruct_from_coeffs(y_test)\n",
    "\n",
    "true_val = scaler_y.inverse_transform(true_val)\n",
    "true_test = scaler_y.inverse_transform(true_test)\n",
    "\n",
    "y_val_flat = true_val.flatten()\n",
    "y_test_flat = true_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80336881",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_val = reconstruct_from_coeffs(val_pred_np)\n",
    "reconstructed_val = scaler_y.inverse_transform(reconstructed_val)\n",
    "\n",
    "reconstructed_test = reconstruct_from_coeffs(test_pred_np)\n",
    "reconstructed_test = scaler_y.inverse_transform(reconstructed_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_val_flat = reconstructed_val.flatten()\n",
    "recon_test_flat = reconstructed_test.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf871081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datetime index from original DataFrame\n",
    "start_date = df.index.min()\n",
    "end_date = df.index.max()\n",
    "all_datetimes = pd.date_range(start=start_date, end=end_date, freq='h')\n",
    "N_total = len(all_datetimes)\n",
    "\n",
    "# Recalculate split indices\n",
    "train_size = int(0.7 * N_total)\n",
    "val_size = int(0.15 * N_total)\n",
    "val_start = train_size\n",
    "val_end = train_size + val_size\n",
    "test_start = val_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datetime index for predictions\n",
    "val_index_expanded = pd.date_range(start=all_datetimes[val_start], periods=len(y_val_flat), freq='h')\n",
    "test_index_expanded = pd.date_range(start=all_datetimes[test_start], periods=len(y_test_flat), freq='h')\n",
    "\n",
    "# Create DataFrames for evaluation\n",
    "X_val_df = pd.DataFrame(index=val_index_expanded)\n",
    "X_test_df = pd.DataFrame(index=test_index_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e722ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_predictions(\n",
    "    y_true, y_pred, df_index,\n",
    "    start_time=\"2023-08-01 00:00:00\",\n",
    "    n_hours=500,\n",
    "    error_threshold=15\n",
    "):\n",
    "    # Convert inputs\n",
    "    start_time = pd.to_datetime(start_time)\n",
    "\n",
    "    # Build aligned DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'actual': y_true,\n",
    "        'predicted': y_pred\n",
    "    }, index=pd.to_datetime(df_index))\n",
    "\n",
    "    # Slice the time range\n",
    "    df_slice = df.loc[start_time : start_time + pd.Timedelta(hours=n_hours)]\n",
    "\n",
    "    # Calculate error\n",
    "    df_slice['error'] = abs(df_slice['actual'] - df_slice['predicted'])\n",
    "\n",
    "    # Identify high error regions\n",
    "    high_error_mask = df_slice['error'] > error_threshold\n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Actual values (Deep blue)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_slice.index, y=df_slice['actual'],\n",
    "        mode='lines', name='True',\n",
    "        line=dict(color='#1f77b4', width=2)\n",
    "    ))\n",
    "\n",
    "    # Predicted values (Soft green)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_slice.index, y=df_slice['predicted'],\n",
    "        mode='lines', name='Predicted',\n",
    "        line=dict(color='#2ca02c', width=2)\n",
    "    ))\n",
    "\n",
    "    # High-error markers (Bold red)\n",
    "    if high_error_mask.any():\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_slice.index[high_error_mask],\n",
    "            y=df_slice['predicted'][high_error_mask],\n",
    "            mode='markers',\n",
    "            marker=dict(size=6, color='#d62728', symbol='circle'),\n",
    "            name=f'Error > {error_threshold}',\n",
    "            text=[f\"Error: {e:.2f}\" for e in df_slice['error'][high_error_mask]],  # Hover text\n",
    "            hoverinfo='text+x+y',\n",
    "            showlegend=True\n",
    "        ))\n",
    "\n",
    "    # Layout styling\n",
    "    fig.update_layout(\n",
    "        title=f\"LSTM Forecast from {start_time.strftime('%Y-%m-%d %H:%M')} ({n_hours} hours)\",\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=\"Value\",\n",
    "        template=\"plotly_white\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "        height=500,\n",
    "        margin=dict(l=40, r=40, t=60, b=40)\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_predictions(\n",
    "#     y_true=y_val_flat,\n",
    "#     y_pred=recon_val_flat,  # Use the reconstructed, inverse-scaled prediction\n",
    "#     df_index=X_val_df.index,  # Assuming you have this aligned with y_val\n",
    "#     start_time=\"2023-08-01 00:00:00\",\n",
    "#     n_hours=500\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c0d771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Metric        MAE        DAE        RMSE        R2  \\\n",
      "0  Validation Set  68.788321  37.506116   85.937986 -0.019076   \n",
      "1        Test Set  74.072805  63.383894  104.959034 -0.295580   \n",
      "\n",
      "   Lower Predictions (%)  \n",
      "0              43.897201  \n",
      "1              66.067470  \n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate model with reconstructed predictions ---\n",
    "results = evaluate_lstm(\n",
    "    y_val=y_val_flat,\n",
    "    y_val_pred=recon_val_flat,       # updated\n",
    "    y_test=y_test_flat,\n",
    "    y_test_pred=recon_test_flat,     # updated\n",
    "    X_val=X_val_df,\n",
    "    X_test=X_test_df\n",
    ")\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36114238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 68.788\n",
      "RMSE: 85.938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "mae = mean_absolute_error(y_val_flat, recon_val_flat)\n",
    "rmse = root_mean_squared_error(y_val_flat, recon_val_flat)\n",
    "\n",
    "print(f\"MAE: {mae:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d202f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_datetime(model, df, timestamp, n_steps=48, target_col='Price (USD/MWh)', scaler_y=None):\n",
    "    \"\"\"\n",
    "    Predict 24-hour prices starting from a given timestamp.\n",
    "\n",
    "    Args:\n",
    "        model: Trained LSTMForecast model.\n",
    "        df: Preprocessed + scaled DataFrame (with DateTime index).\n",
    "        timestamp: Datetime string or pd.Timestamp (e.g. '2023-01-01 00:00').\n",
    "        n_steps: Number of past hours to use (default = 48).\n",
    "        target_col: Name of target column.\n",
    "        scaler_y: Scaler used for the target column.\n",
    "\n",
    "    Returns:\n",
    "        List of (datetime, predicted_price) tuples.\n",
    "    \"\"\"\n",
    "    if isinstance(timestamp, str):\n",
    "        timestamp = pd.Timestamp(timestamp)\n",
    "        \n",
    "    # Check if enough history is available\n",
    "    start_idx = df.index.get_loc(timestamp)\n",
    "    if start_idx < n_steps:\n",
    "        raise ValueError(\"Not enough history before this timestamp.\")\n",
    "\n",
    "    # Build the input sequence (excluding target columns)\n",
    "    seq_df = df.iloc[start_idx - n_steps:start_idx].drop(columns=[target_col, 'target_scaled'])\n",
    "    seq_input = seq_df.values  # shape: (n_steps, num_features)\n",
    "\n",
    "    # Predict the future prices\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(seq_input[np.newaxis, :, :], dtype=torch.float32)\n",
    "        y_pred = model(x).squeeze().numpy()  # shape: (forecast_horizon, )\n",
    "\n",
    "    # Reverse scaling (if provided)\n",
    "    if scaler_y is not None:\n",
    "        y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1))  # Inverse transform predictions\n",
    "        y_pred_original_flat = y_pred_original.flatten()  # Flatten to 1D\n",
    "    else:\n",
    "        y_pred_original_flat = y_pred  # If no scaler, use raw predictions\n",
    "\n",
    "    # Build future timestamps\n",
    "    future_times = [timestamp + pd.Timedelta(hours=i) for i in range(24)]\n",
    "    \n",
    "    return list(zip(future_times, y_pred_original_flat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef25fe44",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 37, got 33",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m preds = \u001b[43mpredict_from_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2024-01-01 00:00\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mscaler_y\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaler_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t, price \u001b[38;5;129;01min\u001b[39;00m preds:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprice\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[131]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mpredict_from_datetime\u001b[39m\u001b[34m(model, df, timestamp, n_steps, target_col, scaler_y)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     31\u001b[39m     x = torch.tensor(seq_input[np.newaxis, :, :], dtype=torch.float32)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     y_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.squeeze().numpy()  \u001b[38;5;66;03m# shape: (forecast_horizon, )\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Reverse scaling (if provided)\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scaler_y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mWT_LSTMForecast.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     lstm_out, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, seq_len, hidden_size]\u001b[39;00m\n\u001b[32m     21\u001b[39m     lstm_out = lstm_out[:, -\u001b[38;5;28mself\u001b[39m.forecast_horizon:, :]  \u001b[38;5;66;03m# Slicing to get the last 'forecast_horizon' time steps\u001b[39;00m\n\u001b[32m     22\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.fc(lstm_out)  \u001b[38;5;66;03m# [batch, forecast_horizon, output_dim]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1101\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1093\u001b[39m     c_zeros = torch.zeros(\n\u001b[32m   1094\u001b[39m         \u001b[38;5;28mself\u001b[39m.num_layers * num_directions,\n\u001b[32m   1095\u001b[39m         max_batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1098\u001b[39m         device=\u001b[38;5;28minput\u001b[39m.device,\n\u001b[32m   1099\u001b[39m     )\n\u001b[32m   1100\u001b[39m     hx = (h_zeros, c_zeros)\n\u001b[32m-> \u001b[39m\u001b[32m1101\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1002\u001b[39m, in \u001b[36mLSTM.check_forward_args\u001b[39m\u001b[34m(self, input, hidden, batch_sizes)\u001b[39m\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_forward_args\u001b[39m(\n\u001b[32m    997\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    998\u001b[39m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[32m    999\u001b[39m     hidden: \u001b[38;5;28mtuple\u001b[39m[Tensor, Tensor],  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m   1000\u001b[39m     batch_sizes: Optional[Tensor],\n\u001b[32m   1001\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_hidden_size(\n\u001b[32m   1004\u001b[39m         hidden[\u001b[32m0\u001b[39m],\n\u001b[32m   1005\u001b[39m         \u001b[38;5;28mself\u001b[39m.get_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[32m   1006\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1007\u001b[39m     )\n\u001b[32m   1008\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_hidden_size(\n\u001b[32m   1009\u001b[39m         hidden[\u001b[32m1\u001b[39m],\n\u001b[32m   1010\u001b[39m         \u001b[38;5;28mself\u001b[39m.get_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[32m   1011\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1012\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/nn/modules/rnn.py:314\u001b[39m, in \u001b[36mRNNBase.check_input\u001b[39m\u001b[34m(self, input, batch_sizes)\u001b[39m\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    311\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.dim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    312\u001b[39m     )\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.input_size != \u001b[38;5;28minput\u001b[39m.size(-\u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    315\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.input_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size(-\u001b[32m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    316\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: input.size(-1) must be equal to input_size. Expected 37, got 33"
     ]
    }
   ],
   "source": [
    "# preds = predict_from_datetime(model, df_scaled, '2024-01-01 00:00',scaler_y=scaler_y)\n",
    "\n",
    "# for t, price in preds:\n",
    "#     print(f\"{t}: ${price:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984f399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd979aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6a6b6a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8df8b64c",
   "metadata": {},
   "source": [
    "#  Post-Hoc Interpretability with Integrated Gradients (XAI)\n",
    "To understand how the LSTM model arrives at its predictions, we apply post-hoc interpretability methods—techniques used after model training to explain behavior without altering the model itself. One such method is Integrated Gradients, which attributes importance scores to input features based on their contribution to a specific prediction. This approach is particularly valuable for complex, black-box models like LSTMs, where internal mechanisms are not easily interpretable. By applying Integrated Gradients, we gain insights into both individual decisions (local interpretation) and broader model behavior (global interpretation), helping to build transparency, trust, and accountability in the forecasting process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc6c26b",
   "metadata": {},
   "source": [
    "## Local Interpretation\n",
    "The local interpretation using Integrated Gradients provides insight into which input features most influenced the model’s prediction for a specific timestamp. By attributing importance values to each feature in that one sample, we can understand the direction (positive or negative) and magnitude of their impact on the predicted price. This helps explain the model's reasoning at an individual decision level — for example, highlighting that high 7-day volatility or weekend timing pushed the forecast upward in that particular context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "167851ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Mismatch between feature count and column names!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# --- Match feature names to X_val's input dimensions ---\u001b[39;00m\n\u001b[1;32m     10\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m df_lstm_input\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcA_3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcD_3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcD_2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcD_1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m X_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(feature_names), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch between feature count and column names!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# --- Disable cuDNN for RNN + IG compatibility ---\u001b[39;00m\n\u001b[1;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Mismatch between feature count and column names!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "# --- Make sure your model is in evaluation mode ---\n",
    "model.eval()\n",
    "\n",
    "# --- Match feature names to X_val's input dimensions ---\n",
    "feature_names = df_lstm_input.columns.drop(['cA_3', 'cD_3', 'cD_2', 'cD_1'])\n",
    "assert X_val.shape[2] == len(feature_names), \"Mismatch between feature count and column names!\"\n",
    "\n",
    "# --- Disable cuDNN for RNN + IG compatibility ---\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# --- Initialize Integrated Gradients ---\n",
    "ig = IntegratedGradients(model)\n",
    "output_dims = 4  # cA3, cD3, cD2, cD1\n",
    "time_step = 0    # Forecast timestep to evaluate (e.g. first timestep of forecast horizon)\n",
    "\n",
    "all_attributions = []\n",
    "\n",
    "# Loop over validation samples\n",
    "for i in range(len(X_val)):\n",
    "    input_tensor = torch.tensor(X_val[i:i+1], dtype=torch.float32, requires_grad=True).to(next(model.parameters()).device)\n",
    "\n",
    "    sample_attr = []\n",
    "\n",
    "    # Compute attribution for each of the 4 wavelet coefficients\n",
    "    for coeff_idx in range(output_dims):\n",
    "        model.train()  # Enables backward path through LSTM\n",
    "        attr = ig.attribute(\n",
    "            input_tensor,\n",
    "            target=(time_step, coeff_idx),\n",
    "            return_convergence_delta=False\n",
    "        )\n",
    "        model.eval()\n",
    "\n",
    "        # Sum over time dimension to collapse into (features,)\n",
    "        attr = attr.sum(dim=1).squeeze().detach().cpu().numpy()\n",
    "        sample_attr.append(attr)\n",
    "\n",
    "    # Stack attributions: shape (4, num_features)\n",
    "    sample_attr = np.stack(sample_attr)\n",
    "    all_attributions.append(sample_attr)\n",
    "\n",
    "# Re-enable cuDNN\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# --- Aggregate attributions ---\n",
    "# Shape: (num_samples, 4, num_features)\n",
    "all_attr = np.stack(all_attributions)\n",
    "\n",
    "# Global average over all wavelet outputs and all samples\n",
    "avg_attr = np.mean(all_attr, axis=(0, 1))  # shape: (num_features,)\n",
    "\n",
    "# --- Plot global feature importance ---\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(feature_names, avg_attr)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Average Attribution\")\n",
    "plt.title(\"Global Feature Importance (Averaged over Coefficients and Samples)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee1e07",
   "metadata": {},
   "source": [
    "The global interpretation aggregates feature attributions across many samples to show which inputs the model relies on most consistently. By averaging importance scores, it highlights the overall influence of each feature on predictions—revealing patterns such as persistent reliance on long-term volatility or cyclical time features. This offers a high-level understanding of the model’s behavior and helps validate that it aligns with domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cd698",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attributions = []\n",
    "model.eval()\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    input_tensor = torch.tensor(X_val[i:i+1], dtype=torch.float32, requires_grad=True)\n",
    "    attributions, _ = ig.attribute(input_tensor, target=0, return_convergence_delta=True)\n",
    "    summed = attributions.sum(dim=1).squeeze().detach().numpy()\n",
    "    all_attributions.append(summed)\n",
    "\n",
    "# Now average across all samples\n",
    "avg_attr = np.mean(np.stack(all_attributions), axis=0)\n",
    "\n",
    "# Plot global feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(feature_names, avg_attr)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Average Attribution\")\n",
    "plt.title(\"Global Feature Importance via Integrated Gradients\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
