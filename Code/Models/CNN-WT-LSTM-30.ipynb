{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f136a04",
   "metadata": {},
   "source": [
    "This file will run a CNN-WT-LSTM model with a 30 day lookback to predict the electricity prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b03c0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import plot_comparison, evaluate_lstm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26a92140",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Data/zra_sgp_dam.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0032cb70",
   "metadata": {},
   "source": [
    "# Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61e6a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cyclic(df, col, max_val):\n",
    "    \"\"\"\"\n",
    "    Time features like Hour, Month, day_of_week are cyclical, not linear. \n",
    "    Without encoding them properly, the model will misunderstand their relationships.\n",
    "    \"\"\"\n",
    "    df[col + '_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "    df[col + '_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "    return df\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Cyclic encode time features\n",
    "    df = encode_cyclic(df, 'Hour', 24)\n",
    "    df = encode_cyclic(df, 'Month', 12)\n",
    "    df = encode_cyclic(df, 'day_of_week', 7)\n",
    "    \n",
    "    # Drop unused or problematic columns\n",
    "    df = df.drop(columns=['Hour', 'Month', 'day_of_week'])  # Keep cyclic versions instead\n",
    "    \n",
    "    # Fill/clean if needed\n",
    "    df = df.fillna(method='ffill').dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96285121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequence for LSTM adapted for WT\n",
    "def create_sequences_wt(data, target_col, lookback=48, forecast_horizon=24):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - lookback - forecast_horizon):\n",
    "        X_seq = data.iloc[i:i + lookback].values  # Use the lookback period for X\n",
    "        y_seq = data.iloc[i + lookback:i + lookback + forecast_horizon][target_col].values  # Multi-target for y\n",
    "        X.append(X_seq)\n",
    "        y.append(y_seq)\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61e10133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7508771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WT_LSTMForecast(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2,\n",
    "                 forecast_horizon=24, output_dim=4):  # output_dim corresponds to number of coefficients you're forecasting\n",
    "        super().__init__()\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # [batch, seq_len, hidden_size]\n",
    "        lstm_out = lstm_out[:, -self.forecast_horizon:, :]  # Slicing to get the last 'forecast_horizon' time steps\n",
    "        out = self.fc(lstm_out)  # [batch, forecast_horizon, output_dim]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e8b35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.log(torch.cosh(y_pred - y_true + 1e-12))  # added epsilon to prevent log(0)\n",
    "        return torch.mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9584fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, val_dataloader=None, epochs=10, lr=1e-3, patience=10, min_delta=1e-4):\n",
    "    device = next(model.parameters()).device\n",
    "    criterion = LogCoshLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = copy.deepcopy(model.state_dict())\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        loss_history.append(avg_loss)\n",
    "\n",
    "        if val_dataloader:\n",
    "            model.eval()\n",
    "            val_total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_dataloader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    pred = model(xb)\n",
    "                    loss = criterion(pred, yb)\n",
    "                    val_total_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = val_total_loss / len(val_dataloader)\n",
    "            val_loss_history.append(avg_val_loss)\n",
    "\n",
    "            if best_val_loss - avg_val_loss > min_delta:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"⏹️ Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if val_dataloader:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return loss_history, val_loss_history if val_dataloader else loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "556c9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_history):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(loss_history, label='Training loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f56ff6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_importance(importances, feature_names=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    names = list(feature_names) if feature_names is not None else [f\"Feature {i}\" for i in range(len(importances))]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(importances)), importances[indices])\n",
    "    plt.xticks(range(len(importances)), [names[i] for i in indices], rotation=45, ha='right')\n",
    "    plt.title(\"SHAP Feature Importance (Averaged)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4f5eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_datetime(model, df, timestamp, n_steps=48, target_col='Price (USD/MWh)', scaler_y=None):\n",
    "    \"\"\"\n",
    "    Predict 24-hour prices starting from a given timestamp.\n",
    "\n",
    "    Args:\n",
    "        model: Trained LSTMForecast model.\n",
    "        df: Preprocessed + scaled DataFrame (with DateTime index).\n",
    "        timestamp: Datetime string or pd.Timestamp (e.g. '2023-01-01 00:00').\n",
    "        n_steps: Number of past hours to use (default = 48).\n",
    "        target_col: Name of target column.\n",
    "        scaler_y: Scaler used for the target column.\n",
    "\n",
    "    Returns:\n",
    "        List of (datetime, predicted_price) tuples.\n",
    "    \"\"\"\n",
    "    if isinstance(timestamp, str):\n",
    "        timestamp = pd.Timestamp(timestamp)\n",
    "        \n",
    "    # Check if enough history is available\n",
    "    start_idx = df.index.get_loc(timestamp)\n",
    "    if start_idx < n_steps:\n",
    "        raise ValueError(\"Not enough history before this timestamp.\")\n",
    "\n",
    "    # Build the input sequence (excluding target columns)\n",
    "    seq_df = df.iloc[start_idx - n_steps:start_idx].drop(columns=[target_col, 'target_scaled'])\n",
    "    seq_input = seq_df.values  # shape: (n_steps, num_features)\n",
    "\n",
    "    # Predict the future prices\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(seq_input[np.newaxis, :, :], dtype=torch.float32)\n",
    "        y_pred = model(x).squeeze().numpy()  # shape: (forecast_horizon, )\n",
    "\n",
    "    # Reverse scaling (if provided)\n",
    "    if scaler_y is not None:\n",
    "        y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1))  # Inverse transform predictions\n",
    "        y_pred_original_flat = y_pred_original.flatten()  # Flatten to 1D\n",
    "    else:\n",
    "        y_pred_original_flat = y_pred  # If no scaler, use raw predictions\n",
    "\n",
    "    # Build future timestamps\n",
    "    future_times = [timestamp + pd.Timedelta(hours=i) for i in range(24)]\n",
    "    \n",
    "    return list(zip(future_times, y_pred_original_flat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aed1bc",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca40c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess (make sure 'Date' becomes index)\n",
    "df_clean = preprocess(df)\n",
    "\n",
    "# Define target\n",
    "target_col = 'Price (USD/MWh)'\n",
    "features = df_clean.drop(columns=[target_col])\n",
    "target = df_clean[target_col]\n",
    "\n",
    "# Columns by type\n",
    "minmax_cols = ['Tati- normalised output', 'E_Grid (Mw)', 'Revenues (USD)', \n",
    "               'Flow_chavuma', 'Level_kariba', 'Flow_nana']\n",
    "standard_cols = ['Volatility_1 Day', 'Volatility_3 Days', 'Volatility_7 Days', 'Volatility_30 Days',\n",
    "                 'roc_49h', 'momentum_49h']\n",
    "no_scaling_cols = ['Hour_sin', 'Hour_cos', 'Month_sin', 'Month_cos',\n",
    "                   'day_of_week_sin', 'day_of_week_cos']\n",
    "\n",
    "# Initialize scalers\n",
    "minmax_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Copy clean DataFrame\n",
    "df_scaled = df_clean.copy()\n",
    "\n",
    "# Apply scalers to appropriate columns\n",
    "df_scaled[minmax_cols] = minmax_scaler.fit_transform(df_clean[minmax_cols])\n",
    "df_scaled[standard_cols] = standard_scaler.fit_transform(df_clean[standard_cols])\n",
    "\n",
    "# Target scaling (fit only on the column, keep shape)\n",
    "df_scaled[\"target_scaled\"] = scaler_y.fit_transform(df_clean[[target_col]])\n",
    "\n",
    "# Optionally retain unscaled target for reference\n",
    "df_scaled[target_col] = target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e038617",
   "metadata": {},
   "source": [
    "# CNN\n",
    "CNN-based feature extractor before creating sequences for your time series model.\n",
    "Use a 1D CNN to learn temporal features from the multivariate time series, and combine those with your original engineered features (like Fourier features, volatility, etc.) to build an enriched LSTM input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "665894bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels=16, kernel_size=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, out_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels, sequence_length)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        return x  # shape: (batch, out_channels, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a0b6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_cnn_over_time(df_features, window=48, stride=1, device='cpu'):\n",
    "    \"\"\"\n",
    "    Apply CNN over rolling time windows of multivariate input.\n",
    "    Returns a DataFrame with CNN features per time step (centered).\n",
    "    \"\"\"\n",
    "    feature_cols = df_features.columns\n",
    "    num_features = len(feature_cols)\n",
    "    num_windows = (len(df_features) - window) // stride + 1\n",
    "\n",
    "    # Prepare input tensor\n",
    "    X_cnn = []\n",
    "    for i in range(0, len(df_features) - window + 1, stride):\n",
    "        window_data = df_features.iloc[i:i+window].values.T  # shape: (features, window)\n",
    "        X_cnn.append(window_data)\n",
    "\n",
    "    X_cnn = torch.tensor(np.stack(X_cnn), dtype=torch.float32).to(device)  # shape: (batch, features, window)\n",
    "\n",
    "    # CNN model\n",
    "    cnn = CNNFeatureExtractor(input_channels=num_features).to(device)\n",
    "    cnn.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features_out = cnn(X_cnn)  # shape: (batch, out_channels, window)\n",
    "    \n",
    "    # Collapse time dimension (e.g., take mean over time)\n",
    "    pooled = features_out.mean(dim=2).cpu().numpy()  # shape: (batch, out_channels)\n",
    "\n",
    "    # Align timestamps to center of each window\n",
    "    start_index = window // 2\n",
    "    end_index = start_index + len(pooled)\n",
    "    timestamps = df_features.index[start_index:end_index]\n",
    "\n",
    "    return pd.DataFrame(pooled, index=timestamps, columns=[f'cnn_feat_{i}' for i in range(pooled.shape[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a854f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_features_df = apply_cnn_over_time(\n",
    "    df_scaled[minmax_cols + standard_cols + no_scaling_cols],  # all features\n",
    "    window=48,\n",
    "    stride=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e315c4",
   "metadata": {},
   "source": [
    "# Applying WT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3338055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align target with features\n",
    "cnn_features_df[\"target_scaled\"] = df_scaled[\"target_scaled\"].loc[cnn_features_df.index]\n",
    "df_model_input = cnn_features_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba13c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "\n",
    "# ---- Step 1: Extract normalized target as numpy array ----\n",
    "target_array = df_model_input[\"target_scaled\"].values\n",
    "\n",
    "# ---- Step 2: Apply single-level Wavelet Decomposition ----\n",
    "# --- Parameters ---\n",
    "wavelet = 'db1'       # You can experiment with other wavelets\n",
    "level = 3             # Number of decomposition levels\n",
    "\n",
    "# --- Apply Multi-Level DWT ---\n",
    "coeffs = pywt.wavedec(target_array, wavelet=wavelet, level=level)\n",
    "\n",
    "# Denoising (optional)\n",
    "threshold = np.std(coeffs[-1]) * np.sqrt(2 * np.log(len(target_array)))\n",
    "denoised_coeffs = [coeffs[0]]  # Approximation stays unchanged\n",
    "for cd in coeffs[1:]:\n",
    "    denoised_cd = pywt.threshold(cd, threshold, mode='soft')\n",
    "    denoised_coeffs.append(denoised_cd)\n",
    "\n",
    "# --- Reconstruct signal (optional step, useful for visualization) ---\n",
    "signal_denoised = pywt.waverec(denoised_coeffs, wavelet=wavelet)\n",
    "\n",
    "# --- Format features ---\n",
    "def pad_or_truncate(arr, target_len):\n",
    "    if len(arr) < target_len:\n",
    "        return np.pad(arr, (0, target_len - len(arr)), mode='constant')\n",
    "    else:\n",
    "        return arr[:target_len]\n",
    "\n",
    "min_len = min(len(c) for c in coeffs)\n",
    "features_wt = np.column_stack([pad_or_truncate(c, min_len) for c in coeffs])\n",
    "feature_names = [f'cA_{level}'] + [f'cD_{i}' for i in range(level, 0, -1)]\n",
    "df_wt = pd.DataFrame(features_wt, columns=feature_names)\n",
    "\n",
    "# Downsample explanatory features to match min_len\n",
    "features_only = df_scaled.drop(columns=[target_col, \"target_scaled\"]).values\n",
    "factor = len(features_only) // min_len\n",
    "features_downsampled = features_only[:factor * min_len].reshape(min_len, factor, features_only.shape[1]).mean(axis=1)\n",
    "\n",
    "# Combine downsampled features with WT features\n",
    "df_lstm_input = pd.DataFrame(\n",
    "    np.hstack([features_downsampled, df_wt.values]),\n",
    "    columns=[*df_scaled.drop(columns=[target_col, \"target_scaled\"]).columns, *feature_names]\n",
    ")\n",
    "\n",
    "# Inspect final input\n",
    "print(\"LSTM input shape:\", df_lstm_input.shape)\n",
    "df_lstm_input.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d876fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 24\n",
    "lookback = 24 * 30 # 60 days of lookback\n",
    "\n",
    "# Define features and targets explicitly\n",
    "features = df_lstm_input.drop(columns=['cA_3', 'cD_3', 'cD_2', 'cD_1'])  # <- drop wavelet components\n",
    "targets  = df_lstm_input[['cA_3', 'cD_3', 'cD_2', 'cD_1']]               # <- keep as targets\n",
    "\n",
    "# Create X and y\n",
    "X, _ = create_sequences_wt(features, target_col=features.columns[0], lookback=lookback, forecast_horizon=forecast_horizon)\n",
    "_, y_cA = create_sequences_wt(df_lstm_input, target_col='cA_3', lookback=lookback, forecast_horizon=forecast_horizon)\n",
    "_, y_d3 = create_sequences_wt(df_lstm_input, target_col='cD_3', lookback=lookback, forecast_horizon=forecast_horizon)\n",
    "_, y_d2 = create_sequences_wt(df_lstm_input, target_col='cD_2', lookback=lookback, forecast_horizon=forecast_horizon)\n",
    "_, y_d1 = create_sequences_wt(df_lstm_input, target_col='cD_1', lookback=lookback, forecast_horizon=forecast_horizon)\n",
    "\n",
    "y = np.stack([y_cA, y_d3, y_d2, y_d1], axis=-1)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Create datasets\n",
    "train_ds = PriceDataset(X_train, y_train)\n",
    "val_ds = PriceDataset(X_val, y_val)\n",
    "test_ds = PriceDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=False, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d531474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import uniform\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming LSTMForecast and train_ds are defined elsewhere\n",
    "# Also assuming val_loader is defined somewhere\n",
    "\n",
    "input_size = X.shape[2]  # Number of features (should match the input size of the LSTM)\n",
    "\n",
    "class WOA:\n",
    "    def __init__(self, n_whales, max_iter, bounds):\n",
    "        self.n_whales = n_whales\n",
    "        self.max_iter = max_iter\n",
    "        self.bounds = bounds\n",
    "        self.population = self.init_population()\n",
    "        self.best_whale = None\n",
    "        self.best_fitness = float('inf')\n",
    "\n",
    "    def init_population(self):\n",
    "        \"\"\"Initialize whale population randomly within the specified bounds.\"\"\"\n",
    "        population = []\n",
    "        for _ in range(self.n_whales):\n",
    "            whale = [uniform(*self.bounds[param]) for param in self.bounds]\n",
    "            population.append(whale)\n",
    "        return population\n",
    "\n",
    "    def smape(self, y_true, y_pred):\n",
    "        \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "        smape = np.mean(np.abs(y_pred - y_true) / denominator) * 100\n",
    "        return smape\n",
    "    \n",
    "    def evaluate_model(self, model, val_loader, device):\n",
    "        \"\"\"Evaluate the model on the validation set.\"\"\"\n",
    "        model.to(device)  # Ensure model is on the correct device (GPU or CPU)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        with torch.no_grad():  # No gradient calculation needed during evaluation\n",
    "            for xb, yb in val_loader:  # Iterate over validation data\n",
    "                xb, yb = xb.to(device), yb.to(device)  # Move data to the correct device\n",
    "                output = model(xb)  # Model's predictions\n",
    "\n",
    "                y_true.append(yb.cpu().numpy())  # Store actual values\n",
    "                y_pred.append(output.cpu().numpy())  # Store predicted values\n",
    "\n",
    "        # Flatten the lists into 1D arrays for easy SMAPE calculation\n",
    "        y_true = np.concatenate(y_true, axis=0)\n",
    "        y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "        return y_true, y_pred\n",
    "\n",
    "    def fitness(self, whale):\n",
    "        \"\"\"Calculate fitness (SMAPE) of the current whale configuration.\"\"\"\n",
    "        hidden_size, num_layers, dropout, lr = whale\n",
    "\n",
    "        N_subset = min(8760 - lookback, len(train_ds) - lookback)\n",
    "\n",
    "        subset_indices = list(range(N_subset))  # deterministic: first N_subset rows\n",
    "        subset_train_ds = torch.utils.data.Subset(train_ds, subset_indices)\n",
    "        subset_train_loader = DataLoader(subset_train_ds, batch_size=64, shuffle=True)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        model = WT_LSTMForecast(\n",
    "            input_size=input_size,\n",
    "            hidden_size=int(hidden_size),\n",
    "            num_layers=int(num_layers),\n",
    "            dropout=dropout,\n",
    "            output_dim=4\n",
    "        ).to(device)  # Move the model to the correct device\n",
    "        start_time = time.time()\n",
    "        train_losses, _ = train_model(\n",
    "            model,\n",
    "            dataloader=subset_train_loader,\n",
    "            val_dataloader=val_loader,\n",
    "            epochs=200,\n",
    "            lr=lr,\n",
    "            patience=20,\n",
    "            min_delta=1e-4\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"\\nTraining completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "        plot_loss(train_losses)\n",
    "        # Get actuals and predictions from validation\n",
    "        y_true, y_pred = self.evaluate_model(model, val_loader, device)\n",
    "\n",
    "        return self.smape(y_true, y_pred)\n",
    "\n",
    "\n",
    "    def update_position(self, whale, a, best_whale):\n",
    "        \"\"\"Update the position of the whale based on the hunting behavior.\"\"\"\n",
    "        A = 2 * a * np.random.rand() - a  # Randomization for exploration\n",
    "        C = 2 * np.random.rand()  # Another randomization factor for exploration\n",
    "        p = np.random.rand()  # Probability for exploitation or exploration\n",
    "\n",
    "        if p < 0.5:\n",
    "            if np.abs(A) >= 1:\n",
    "                # Exploration: Random movement\n",
    "                rand_whale = self.population[np.random.randint(self.n_whales)]\n",
    "                new_whale = np.array(rand_whale) - A * np.abs(C * np.array(rand_whale) - np.array(whale))\n",
    "            else:\n",
    "                # Exploitation: Move towards best whale\n",
    "                new_whale = np.array(best_whale) - A * np.abs(C * np.array(best_whale) - np.array(whale))\n",
    "        else:\n",
    "            distance_best = np.abs(np.array(best_whale) - np.array(whale))\n",
    "            new_whale = distance_best * np.exp(a * distance_best) * np.cos(2 * np.pi * np.random.rand())\n",
    "\n",
    "        # Bound check: Ensure the new whale is within bounds\n",
    "        new_whale = np.clip(new_whale, [self.bounds[param][0] for param in self.bounds],\n",
    "                            [self.bounds[param][1] for param in self.bounds])\n",
    "        \n",
    "        return new_whale\n",
    "\n",
    "    def optimize(self, smape_threshold=10.0):\n",
    "        \"\"\"Run the Whale Optimization Algorithm.\"\"\"\n",
    "        for t in tqdm(range(self.max_iter), desc=\"WOA Iterations\"):\n",
    "            a = 2 - t * (2 / self.max_iter)\n",
    "\n",
    "            for i in tqdm(range(self.n_whales), desc=f\"Whales (Iteration {t+1})\", leave=False):\n",
    "                whale = self.population[i]\n",
    "                hidden_size, num_layers, dropout, lr = whale\n",
    "\n",
    "                # print(f\"Iteration {t+1}, Whale {i+1}: hidden_size={int(hidden_size)}, \"\n",
    "                #       f\"num_layers={int(num_layers)}, dropout={dropout:.3f}, lr={lr:.6f}\")\n",
    "\n",
    "                fitness_val = self.fitness(whale)\n",
    "\n",
    "                if fitness_val < self.best_fitness:\n",
    "                    self.best_fitness = fitness_val\n",
    "                    self.best_whale = whale\n",
    "\n",
    "                    # Early stopping if precision threshold met\n",
    "                    if self.best_fitness <= smape_threshold:\n",
    "                        # print(f\"\\nStopping early at iteration {t+1} with SMAPE {self.best_fitness:.2f}\")\n",
    "                        return self.best_whale, self.best_fitness\n",
    "\n",
    "                self.population[i] = self.update_position(whale, a, self.best_whale)\n",
    "\n",
    "        return self.best_whale, self.best_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb30de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bounds for the hyperparameters\n",
    "bounds = {\n",
    "    'hidden_size': (32, 128),  # Increased hidden size range to 256\n",
    "    'num_layers': (1, 4),      # Increased num_layers range to 4\n",
    "    'dropout': (0.1, 0.5),     # Increased dropout range to 0.5\n",
    "    'lr': (5e-4, 1e-2)         # Increased learning rate range to 1e-2\n",
    "}\n",
    "\n",
    "# Create the WOA optimizer with the updated bounds\n",
    "woa = WOA(n_whales=5, max_iter=4, bounds=bounds)\n",
    "# Run optimization\n",
    "best_params, best_val_loss = woa.optimize()\n",
    "\n",
    "# Output the best found parameters and corresponding validation loss\n",
    "print(\"\\nBest configuration from WOA:\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d58eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Train model with best params from WOA\"\n",
    "import time  # Add this at the top of your script if not already imported\n",
    "# Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = X.shape[2]  # Number of features (should match the input size of the LSTM)\n",
    "\n",
    "model = WT_LSTMForecast(\n",
    "    input_size=input_size,\n",
    "    hidden_size=int(best_params[0]),  # Ensure integer value for hidden_size\n",
    "    num_layers=int(best_params[1]),   # Ensure integer value for num_layers\n",
    "    dropout=int(best_params[2])\n",
    ").to(device)\n",
    "\n",
    "# Train the model with timing\n",
    "start_time = time.time()\n",
    "\n",
    "train_losses, val_losses = train_model(\n",
    "    model,\n",
    "    dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    epochs=500,\n",
    "    lr=best_params[3],\n",
    "    patience=150,       # stop if no val improvement\n",
    "    min_delta=1e-4     # must improve by at least this much\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTraining completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "plot_loss(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af78dd",
   "metadata": {},
   "source": [
    "# Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d0c0a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch model to eval mode\n",
    "model.eval()\n",
    "\n",
    "def get_predictions(model, dataloader):\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in dataloader:\n",
    "            xb = xb.to(device)\n",
    "            preds = model(xb)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "    return np.concatenate(all_preds, axis=0)\n",
    "\n",
    "# Get predictions from validation and test loaders\n",
    "val_pred_np = get_predictions(model, val_loader)  # <-- Now this is defined\n",
    "test_pred_np = get_predictions(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e178d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_from_coeffs(y_pred, wavelet='db1'):\n",
    "    \"\"\"\n",
    "    Reconstruct time series from predicted DWT coefficients.\n",
    "    y_pred: numpy array of shape (samples, forecast_horizon, 4)\n",
    "    Returns:\n",
    "        reconstructed: numpy array of shape (samples, forecast_horizon)\n",
    "    \"\"\"\n",
    "    reconstructed = []\n",
    "    for sample in y_pred:\n",
    "        # sample: (forecast_horizon, 4)\n",
    "        sample_recon = []\n",
    "        for t in range(sample.shape[0]):\n",
    "            cA3, cD3, cD2, cD1 = sample[t]\n",
    "\n",
    "            # Use pywt.waverec to reconstruct from multi-level coeffs\n",
    "            coeffs = [np.array([cA3]), np.array([cD3]), np.array([cD2]), np.array([cD1])]\n",
    "            signal = pywt.waverec(coeffs, wavelet)\n",
    "\n",
    "            # Take only the first value (or mean) to align with time step t\n",
    "            sample_recon.append(signal[0])  # or signal.mean(), etc.\n",
    "\n",
    "        reconstructed.append(sample_recon)\n",
    "\n",
    "    return np.array(reconstructed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5c242339",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = reconstruct_from_coeffs(val_pred_np)\n",
    "reconstructed = scaler_y.inverse_transform(reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "52de5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_val = reconstruct_from_coeffs(y_val)\n",
    "true_test = reconstruct_from_coeffs(y_test)\n",
    "\n",
    "true_val = scaler_y.inverse_transform(true_val)\n",
    "true_test = scaler_y.inverse_transform(true_test)\n",
    "\n",
    "y_val_flat = true_val.flatten()\n",
    "y_test_flat = true_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "80336881",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_val = reconstruct_from_coeffs(val_pred_np)\n",
    "reconstructed_val = scaler_y.inverse_transform(reconstructed_val)\n",
    "\n",
    "reconstructed_test = reconstruct_from_coeffs(test_pred_np)\n",
    "reconstructed_test = scaler_y.inverse_transform(reconstructed_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "44c8abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_val_flat = reconstructed_val.flatten()\n",
    "recon_test_flat = reconstructed_test.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cf871081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datetime index from original DataFrame\n",
    "start_date = df.index.min()\n",
    "end_date = df.index.max()\n",
    "all_datetimes = pd.date_range(start=start_date, end=end_date, freq='h')\n",
    "N_total = len(all_datetimes)\n",
    "\n",
    "# Recalculate split indices\n",
    "train_size = int(0.7 * N_total)\n",
    "val_size = int(0.15 * N_total)\n",
    "val_start = train_size\n",
    "val_end = train_size + val_size\n",
    "test_start = val_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "16fd2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datetime index for predictions\n",
    "val_index_expanded = pd.date_range(start=all_datetimes[val_start], periods=len(y_val_flat), freq='h')\n",
    "test_index_expanded = pd.date_range(start=all_datetimes[test_start], periods=len(y_test_flat), freq='h')\n",
    "\n",
    "# Create DataFrames for evaluation\n",
    "X_val_df = pd.DataFrame(index=val_index_expanded)\n",
    "X_test_df = pd.DataFrame(index=test_index_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f4e722ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_predictions(\n",
    "    y_true, y_pred, df_index,\n",
    "    start_time=\"2023-08-01 00:00:00\",\n",
    "    n_hours=500,\n",
    "    error_threshold=15\n",
    "):\n",
    "    # Convert inputs\n",
    "    start_time = pd.to_datetime(start_time)\n",
    "\n",
    "    # Build aligned DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'actual': y_true,\n",
    "        'predicted': y_pred\n",
    "    }, index=pd.to_datetime(df_index))\n",
    "\n",
    "    # Slice the time range\n",
    "    df_slice = df.loc[start_time : start_time + pd.Timedelta(hours=n_hours)]\n",
    "\n",
    "    # Calculate error\n",
    "    df_slice['error'] = abs(df_slice['actual'] - df_slice['predicted'])\n",
    "\n",
    "    # Identify high error regions\n",
    "    high_error_mask = df_slice['error'] > error_threshold\n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Actual values (Deep blue)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_slice.index, y=df_slice['actual'],\n",
    "        mode='lines', name='True',\n",
    "        line=dict(color='#1f77b4', width=2)\n",
    "    ))\n",
    "\n",
    "    # Predicted values (Soft green)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_slice.index, y=df_slice['predicted'],\n",
    "        mode='lines', name='Predicted',\n",
    "        line=dict(color='#2ca02c', width=2)\n",
    "    ))\n",
    "\n",
    "    # High-error markers (Bold red)\n",
    "    if high_error_mask.any():\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_slice.index[high_error_mask],\n",
    "            y=df_slice['predicted'][high_error_mask],\n",
    "            mode='markers',\n",
    "            marker=dict(size=6, color='#d62728', symbol='circle'),\n",
    "            name=f'Error > {error_threshold}',\n",
    "            text=[f\"Error: {e:.2f}\" for e in df_slice['error'][high_error_mask]],  # Hover text\n",
    "            hoverinfo='text+x+y',\n",
    "            showlegend=True\n",
    "        ))\n",
    "\n",
    "    # Layout styling\n",
    "    fig.update_layout(\n",
    "        title=f\"LSTM Forecast from {start_time.strftime('%Y-%m-%d %H:%M')} ({n_hours} hours)\",\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=\"Value\",\n",
    "        template=\"plotly_white\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "        height=500,\n",
    "        margin=dict(l=40, r=40, t=60, b=40)\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c0d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate model with reconstructed predictions ---\n",
    "results = evaluate_lstm(\n",
    "    y_val=y_val_flat,\n",
    "    y_val_pred=recon_val_flat,       # updated\n",
    "    y_test=y_test_flat,\n",
    "    y_test_pred=recon_test_flat,     # updated\n",
    "    X_val=X_val_df,\n",
    "    X_test=X_test_df\n",
    ")\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "17d202f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_datetime(model, df, timestamp, n_steps=48, target_col='Price (USD/MWh)', scaler_y=None):\n",
    "    \"\"\"\n",
    "    Predict 24-hour prices starting from a given timestamp.\n",
    "\n",
    "    Args:\n",
    "        model: Trained LSTMForecast model.\n",
    "        df: Preprocessed + scaled DataFrame (with DateTime index).\n",
    "        timestamp: Datetime string or pd.Timestamp (e.g. '2023-01-01 00:00').\n",
    "        n_steps: Number of past hours to use (default = 48).\n",
    "        target_col: Name of target column.\n",
    "        scaler_y: Scaler used for the target column.\n",
    "\n",
    "    Returns:\n",
    "        List of (datetime, predicted_price) tuples.\n",
    "    \"\"\"\n",
    "    if isinstance(timestamp, str):\n",
    "        timestamp = pd.Timestamp(timestamp)\n",
    "        \n",
    "    # Check if enough history is available\n",
    "    start_idx = df.index.get_loc(timestamp)\n",
    "    if start_idx < n_steps:\n",
    "        raise ValueError(\"Not enough history before this timestamp.\")\n",
    "\n",
    "    # Build the input sequence (excluding target columns)\n",
    "    seq_df = df.iloc[start_idx - n_steps:start_idx].drop(columns=[target_col, 'target_scaled'])\n",
    "    seq_input = seq_df.values  # shape: (n_steps, num_features)\n",
    "\n",
    "    # Predict the future prices\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(seq_input[np.newaxis, :, :], dtype=torch.float32)\n",
    "        y_pred = model(x).squeeze().numpy()  # shape: (forecast_horizon, )\n",
    "\n",
    "    # Reverse scaling (if provided)\n",
    "    if scaler_y is not None:\n",
    "        y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1))  # Inverse transform predictions\n",
    "        y_pred_original_flat = y_pred_original.flatten()  # Flatten to 1D\n",
    "    else:\n",
    "        y_pred_original_flat = y_pred  # If no scaler, use raw predictions\n",
    "\n",
    "    # Build future timestamps\n",
    "    future_times = [timestamp + pd.Timedelta(hours=i) for i in range(24)]\n",
    "    \n",
    "    return list(zip(future_times, y_pred_original_flat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df8b64c",
   "metadata": {},
   "source": [
    "#  Post-Hoc Interpretability with Integrated Gradients (XAI)\n",
    "To understand how the LSTM model arrives at its predictions, we apply post-hoc interpretability methods—techniques used after model training to explain behavior without altering the model itself. One such method is Integrated Gradients, which attributes importance scores to input features based on their contribution to a specific prediction. This approach is particularly valuable for complex, black-box models like LSTMs, where internal mechanisms are not easily interpretable. By applying Integrated Gradients, we gain insights into both individual decisions (local interpretation) and broader model behavior (global interpretation), helping to build transparency, trust, and accountability in the forecasting process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc6c26b",
   "metadata": {},
   "source": [
    "## Global Interpretation\n",
    "The global interpretation aggregates feature attributions across many samples to show which inputs the model relies on most consistently. By averaging importance scores, it highlights the overall influence of each feature on predictions—revealing patterns such as persistent reliance on long-term volatility or cyclical time features. This offers a high-level understanding of the model’s behavior and helps validate that it aligns with domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167851ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "# --- Make sure your model is in evaluation mode ---\n",
    "model.eval()\n",
    "\n",
    "# --- Match feature names to X_val's input dimensions ---\n",
    "feature_names = features.columns\n",
    "assert X_val.shape[2] == len(feature_names), \"Mismatch between feature count and column names!\"\n",
    "\n",
    "# --- Disable cuDNN for RNN + IG compatibility ---\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# --- Initialize Integrated Gradients ---\n",
    "ig = IntegratedGradients(model)\n",
    "output_dims = 4  # cA3, cD3, cD2, cD1\n",
    "time_step = 0    # Forecast timestep to evaluate (e.g. first timestep of forecast horizon)\n",
    "\n",
    "all_attributions = []\n",
    "\n",
    "# Loop over validation samples\n",
    "for i in range(len(X_val)):\n",
    "    input_tensor = torch.tensor(X_val[i:i+1], dtype=torch.float32, requires_grad=True).to(next(model.parameters()).device)\n",
    "\n",
    "    sample_attr = []\n",
    "\n",
    "    # Compute attribution for each of the 4 wavelet coefficients\n",
    "    for coeff_idx in range(output_dims):\n",
    "        model.train()  # Enables backward path through LSTM\n",
    "        attr = ig.attribute(\n",
    "            input_tensor,\n",
    "            target=(time_step, coeff_idx),\n",
    "            return_convergence_delta=False\n",
    "        )\n",
    "        model.eval()\n",
    "\n",
    "        # Sum over time dimension to collapse into (features,)\n",
    "        attr = attr.sum(dim=1).squeeze().detach().cpu().numpy()\n",
    "        sample_attr.append(attr)\n",
    "\n",
    "    # Stack attributions: shape (4, num_features)\n",
    "    sample_attr = np.stack(sample_attr)\n",
    "    all_attributions.append(sample_attr)\n",
    "\n",
    "# Re-enable cuDNN\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# --- Aggregate attributions ---\n",
    "# Shape: (num_samples, 4, num_features)\n",
    "all_attr = np.stack(all_attributions)\n",
    "\n",
    "# Global average over all wavelet outputs and all samples\n",
    "avg_attr = np.mean(all_attr, axis=(0, 1))  # shape: (num_features,)\n",
    "\n",
    "# --- Plot global feature importance ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(feature_names, avg_attr, color='darkorange')  # Set bar color\n",
    "plt.axhline(0, color='gray', linewidth=0.2, linestyle='-')  # Horizontal line at y=0\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Average Attribution\")\n",
    "plt.title(\"Global Feature Importance for CNN-WT-LSTM (Averaged over Coefficients and Samples)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
