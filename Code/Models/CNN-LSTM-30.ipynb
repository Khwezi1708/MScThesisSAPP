{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f136a04",
   "metadata": {},
   "source": [
    "This file will run a CNN-LSTM model with a 30 day lookback to predict the electricity prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b03c0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import plot_comparison, evaluate_lstm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a92140",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Data/zra_sgp_dam.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0032cb70",
   "metadata": {},
   "source": [
    "# Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e6a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cyclic(df, col, max_val):\n",
    "    \"\"\"\"\n",
    "    Time features like Hour, Month, day_of_week are cyclical, not linear. \n",
    "    Without encoding them properly, the model will misunderstand their relationships.\n",
    "    \"\"\"\n",
    "    df[col + '_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "    df[col + '_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "    return df\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Cyclic encode time features\n",
    "    df = encode_cyclic(df, 'Hour', 24)\n",
    "    df = encode_cyclic(df, 'Month', 12)\n",
    "    df = encode_cyclic(df, 'day_of_week', 7)\n",
    "    \n",
    "    # Drop unused or problematic columns\n",
    "    df = df.drop(columns=['Hour', 'Month', 'day_of_week'])  # Keep cyclic versions instead\n",
    "    \n",
    "    # Fill/clean if needed\n",
    "    df = df.fillna(method='ffill').dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96285121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequence for LSTM\n",
    "def create_sequences(data, target_col, n_steps=48, forecast_horizon=24):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps - forecast_horizon):\n",
    "        seq_x = data.iloc[i:i+n_steps].drop(columns=[target_col]).values\n",
    "        seq_y = data.iloc[i+n_steps:i+n_steps+forecast_horizon][target_col].values\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e10133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7508771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Lstm model\n",
    "class LSTMForecast(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2, output_size=24):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout if num_layers > 1 else 0.0, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x) # Get all outputs\n",
    "        out = output[:, -1, :] # Use output from the last timestep\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e8b35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.log(torch.cosh(y_pred - y_true + 1e-12))  # added epsilon to prevent log(0)\n",
    "        return torch.mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9584fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "def train_model(model, dataloader, val_dataloader=None, epochs=10, lr=1e-3, patience=10, min_delta=1e-4):\n",
    "    device = next(model.parameters()).device\n",
    "    criterion = LogCoshLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = copy.deepcopy(model.state_dict())\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        loss_history.append(avg_loss)\n",
    "\n",
    "        if val_dataloader:\n",
    "            model.eval()\n",
    "            val_total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_dataloader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    pred = model(xb)\n",
    "                    loss = criterion(pred, yb)\n",
    "                    val_total_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = val_total_loss / len(val_dataloader)\n",
    "            val_loss_history.append(avg_val_loss)\n",
    "\n",
    "            if best_val_loss - avg_val_loss > min_delta:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"⏹️ Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if val_dataloader:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return loss_history, val_loss_history if val_dataloader else loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "556c9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_history):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(loss_history, label='Training loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f56ff6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_importance(importances, feature_names=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    names = list(feature_names) if feature_names is not None else [f\"Feature {i}\" for i in range(len(importances))]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(importances)), importances[indices])\n",
    "    plt.xticks(range(len(importances)), [names[i] for i in indices], rotation=45, ha='right')\n",
    "    plt.title(\"SHAP Feature Importance (Averaged)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4f5eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_datetime(model, df, timestamp, n_steps=48, target_col='Price (USD/MWh)', scaler_y=None):\n",
    "    \"\"\"\n",
    "    Predict 24-hour prices starting from a given timestamp.\n",
    "\n",
    "    Args:\n",
    "        model: Trained LSTMForecast model.\n",
    "        df: Preprocessed + scaled DataFrame (with DateTime index).\n",
    "        timestamp: Datetime string or pd.Timestamp (e.g. '2023-01-01 00:00').\n",
    "        n_steps: Number of past hours to use (default = 48).\n",
    "        target_col: Name of target column.\n",
    "        scaler_y: Scaler used for the target column.\n",
    "\n",
    "    Returns:\n",
    "        List of (datetime, predicted_price) tuples.\n",
    "    \"\"\"\n",
    "    if isinstance(timestamp, str):\n",
    "        timestamp = pd.Timestamp(timestamp)\n",
    "        \n",
    "    # Check if enough history is available\n",
    "    start_idx = df.index.get_loc(timestamp)\n",
    "    if start_idx < n_steps:\n",
    "        raise ValueError(\"Not enough history before this timestamp.\")\n",
    "\n",
    "    # Build the input sequence (excluding target columns)\n",
    "    seq_df = df.iloc[start_idx - n_steps:start_idx].drop(columns=[target_col, 'target_scaled'])\n",
    "    seq_input = seq_df.values  # shape: (n_steps, num_features)\n",
    "\n",
    "    # Predict the future prices\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(seq_input[np.newaxis, :, :], dtype=torch.float32)\n",
    "        y_pred = model(x).squeeze().numpy()  # shape: (forecast_horizon, )\n",
    "\n",
    "    # Reverse scaling (if provided)\n",
    "    if scaler_y is not None:\n",
    "        y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1))  # Inverse transform predictions\n",
    "        y_pred_original_flat = y_pred_original.flatten()  # Flatten to 1D\n",
    "    else:\n",
    "        y_pred_original_flat = y_pred  # If no scaler, use raw predictions\n",
    "\n",
    "    # Build future timestamps\n",
    "    future_times = [timestamp + pd.Timedelta(hours=i) for i in range(24)]\n",
    "    \n",
    "    return list(zip(future_times, y_pred_original_flat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aed1bc",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca40c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Preprocess (make sure 'Date' becomes index)\n",
    "df_clean = preprocess(df)\n",
    "\n",
    "# Define target\n",
    "target_col = 'Price (USD/MWh)'\n",
    "features = df_clean.drop(columns=[target_col])\n",
    "target = df_clean[target_col]\n",
    "\n",
    "# Columns by type\n",
    "minmax_cols = ['Tati- normalised output', 'E_Grid (Mw)', 'Revenues (USD)', \n",
    "               'Flow_chavuma', 'Level_kariba', 'Flow_nana']\n",
    "standard_cols = ['Volatility_1 Day', 'Volatility_3 Days', 'Volatility_7 Days', 'Volatility_30 Days',\n",
    "                 'roc_49h', 'momentum_49h']\n",
    "no_scaling_cols = ['Hour_sin', 'Hour_cos', 'Month_sin', 'Month_cos',\n",
    "                   'day_of_week_sin', 'day_of_week_cos']\n",
    "\n",
    "# Initialize scalers\n",
    "minmax_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Copy clean DataFrame\n",
    "df_scaled = df_clean.copy()\n",
    "\n",
    "# Apply scalers to appropriate columns\n",
    "df_scaled[minmax_cols] = minmax_scaler.fit_transform(df_clean[minmax_cols])\n",
    "df_scaled[standard_cols] = standard_scaler.fit_transform(df_clean[standard_cols])\n",
    "\n",
    "# Target scaling (fit only on the column, keep shape)\n",
    "df_scaled[\"target_scaled\"] = scaler_y.fit_transform(df_clean[[target_col]])\n",
    "\n",
    "# Optionally retain unscaled target for reference\n",
    "df_scaled[target_col] = target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e038617",
   "metadata": {},
   "source": [
    "# CNN\n",
    "CNN-based feature extractor before creating sequences for your time series model.\n",
    "Use a 1D CNN to learn temporal features from the multivariate time series, and combine those with your original engineered features (like Fourier features, volatility, etc.) to build an enriched LSTM input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "665894bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels=16, kernel_size=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, out_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels, sequence_length)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        return x  # shape: (batch, out_channels, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a0b6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_cnn_over_time(df_features, window=48, stride=1, device='cpu'):\n",
    "    \"\"\"\n",
    "    Apply CNN over rolling time windows of multivariate input.\n",
    "    Returns a DataFrame with CNN features per time step (centered).\n",
    "    \"\"\"\n",
    "    feature_cols = df_features.columns\n",
    "    num_features = len(feature_cols)\n",
    "    num_windows = (len(df_features) - window) // stride + 1\n",
    "\n",
    "    # Prepare input tensor\n",
    "    X_cnn = []\n",
    "    for i in range(0, len(df_features) - window + 1, stride):\n",
    "        window_data = df_features.iloc[i:i+window].values.T  # shape: (features, window)\n",
    "        X_cnn.append(window_data)\n",
    "\n",
    "    X_cnn = torch.tensor(np.stack(X_cnn), dtype=torch.float32).to(device)  # shape: (batch, features, window)\n",
    "\n",
    "    # CNN model\n",
    "    cnn = CNNFeatureExtractor(input_channels=num_features).to(device)\n",
    "    cnn.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features_out = cnn(X_cnn)  # shape: (batch, out_channels, window)\n",
    "    \n",
    "    # Collapse time dimension (e.g., take mean over time)\n",
    "    pooled = features_out.mean(dim=2).cpu().numpy()  # shape: (batch, out_channels)\n",
    "\n",
    "    # Align timestamps to center of each window\n",
    "    start_index = window // 2\n",
    "    end_index = start_index + len(pooled)\n",
    "    timestamps = df_features.index[start_index:end_index]\n",
    "\n",
    "    return pd.DataFrame(pooled, index=timestamps, columns=[f'cnn_feat_{i}' for i in range(pooled.shape[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a854f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_features_df = apply_cnn_over_time(\n",
    "    df_scaled[minmax_cols + standard_cols + no_scaling_cols],  # all features\n",
    "    window=48,\n",
    "    stride=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3338055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align target with features\n",
    "cnn_features_df[\"target_scaled\"] = df_scaled[\"target_scaled\"].loc[cnn_features_df.index]\n",
    "df_lstm_input = cnn_features_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ee4a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyPriceDataset(Dataset):\n",
    "    def __init__(self, df, target_col, lookback, forecast_horizon):\n",
    "        self.lookback = lookback\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.target_col = target_col\n",
    "\n",
    "        self.features = df.drop(columns=[target_col]).values\n",
    "        self.target = df[target_col].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.lookback - self.forecast_horizon + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx : idx + self.lookback]\n",
    "        y = self.target[idx + self.lookback : idx + self.lookback + self.forecast_horizon]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d876fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 24 * 30 # 90 days of history\n",
    "forecast_horizon = 24\n",
    "\n",
    "# Kernel will crash otherwise due to memory\n",
    "if lookback >= 720:\n",
    "    total_len = len(df_scaled)\n",
    "    train_end = int(0.7 * total_len)\n",
    "    val_end = int(0.85 * total_len)\n",
    "\n",
    "    df_train = df_scaled.drop(columns=['Price (USD/MWh)']).iloc[:train_end]\n",
    "    df_val = df_scaled.drop(columns=['Price (USD/MWh)']).iloc[train_end:]\n",
    "    df_test = df_scaled.drop(columns=['Price (USD/MWh)']).iloc[val_end:]\n",
    "\n",
    "    train_ds = LazyPriceDataset(df_train, target_col='target_scaled', lookback=lookback, forecast_horizon=forecast_horizon)\n",
    "    val_ds   = LazyPriceDataset(df_val,   target_col='target_scaled', lookback=lookback, forecast_horizon=forecast_horizon)\n",
    "    test_ds  = LazyPriceDataset(df_test,  target_col='target_scaled', lookback=lookback, forecast_horizon=forecast_horizon)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=False, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False, num_workers=0)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False, num_workers=0)\n",
    "    # After creating train_loader (lazy or not)\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    # Create a *dummy* X with the right shape but no data copying\n",
    "    X = torch.empty((len(train_loader.dataset),) + sample_batch[0].shape[1:])\n",
    "\n",
    "else:\n",
    "    # Create sequences\n",
    "    X, y = create_sequences(df_scaled.drop(columns=['Price (USD/MWh)']), target_col='target_scaled', n_steps=lookback, forecast_horizon=24)\n",
    "\n",
    "    # Train-validation-test split (70-15-15 split)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
    "\n",
    "    # Create datasets\n",
    "    train_ds = PriceDataset(X_train, y_train)\n",
    "    val_ds = PriceDataset(X_val, y_val)\n",
    "    test_ds = PriceDataset(X_test, y_test)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=False, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d531474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import uniform\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "input_size = X.shape[2]  # Number of features (should match the input size of the LSTM)\n",
    "\n",
    "class WOA:\n",
    "    def __init__(self, n_whales, max_iter, bounds):\n",
    "        self.n_whales = n_whales\n",
    "        self.max_iter = max_iter\n",
    "        self.bounds = bounds\n",
    "        self.population = self.init_population()\n",
    "        self.best_whale = None\n",
    "        self.best_fitness = float('inf')\n",
    "\n",
    "    def init_population(self):\n",
    "        \"\"\"Initialize whale population randomly within the specified bounds.\"\"\"\n",
    "        population = []\n",
    "        for _ in range(self.n_whales):\n",
    "            whale = [uniform(*self.bounds[param]) for param in self.bounds]\n",
    "            population.append(whale)\n",
    "        return population\n",
    "\n",
    "    def smape(self, y_true, y_pred):\n",
    "        \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "        smape = np.mean(np.abs(y_pred - y_true) / denominator) * 100\n",
    "        return smape\n",
    "    \n",
    "    def evaluate_model(self, model, val_loader, device):\n",
    "        \"\"\"Evaluate the model on the validation set.\"\"\"\n",
    "        model.to(device)  # Ensure model is on the correct device (GPU or CPU)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        with torch.no_grad():  # No gradient calculation needed during evaluation\n",
    "            for xb, yb in val_loader:  # Iterate over validation data\n",
    "                xb, yb = xb.to(device), yb.to(device)  # Move data to the correct device\n",
    "                output = model(xb)  # Model's predictions\n",
    "\n",
    "                y_true.append(yb.cpu().numpy())  # Store actual values\n",
    "                y_pred.append(output.cpu().numpy())  # Store predicted values\n",
    "\n",
    "        # Flatten the lists into 1D arrays for easy SMAPE calculation\n",
    "        y_true = np.concatenate(y_true, axis=0)\n",
    "        y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "        return y_true, y_pred\n",
    "\n",
    "    def fitness(self, whale):\n",
    "        \"\"\"Calculate fitness (SMAPE) of the current whale configuration.\"\"\"\n",
    "        hidden_size, num_layers, dropout, lr = whale\n",
    "\n",
    "        N_subset = min(8760 - lookback, len(train_ds) - lookback)\n",
    "\n",
    "        subset_indices = list(range(N_subset))  # deterministic: first N_subset rows\n",
    "        subset_train_ds = torch.utils.data.Subset(train_ds, subset_indices)\n",
    "        subset_train_loader = DataLoader(subset_train_ds, batch_size=64, shuffle=True)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        model = LSTMForecast(\n",
    "            input_size=input_size,\n",
    "            hidden_size=int(hidden_size),\n",
    "            num_layers=int(num_layers),\n",
    "            dropout=dropout\n",
    "        ).to(device)  # Move the model to the correct device\n",
    "        start_time = time.time()\n",
    "        train_losses, _ = train_model(\n",
    "            model,\n",
    "            dataloader=subset_train_loader,\n",
    "            val_dataloader=val_loader,\n",
    "            epochs=200,\n",
    "            lr=lr,\n",
    "            patience=20,\n",
    "            min_delta=1e-4\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"\\nTraining completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "        # Get actuals and predictions from validation\n",
    "        y_true, y_pred = self.evaluate_model(model, val_loader, device)\n",
    "\n",
    "        return self.smape(y_true, y_pred)\n",
    "\n",
    "\n",
    "    def update_position(self, whale, a, best_whale):\n",
    "        \"\"\"Update the position of the whale based on the hunting behavior.\"\"\"\n",
    "        A = 2 * a * np.random.rand() - a  # Randomization for exploration\n",
    "        C = 2 * np.random.rand()  # Another randomization factor for exploration\n",
    "        p = np.random.rand()  # Probability for exploitation or exploration\n",
    "\n",
    "        if p < 0.5:\n",
    "            if np.abs(A) >= 1:\n",
    "                # Exploration: Random movement\n",
    "                rand_whale = self.population[np.random.randint(self.n_whales)]\n",
    "                new_whale = np.array(rand_whale) - A * np.abs(C * np.array(rand_whale) - np.array(whale))\n",
    "            else:\n",
    "                # Exploitation: Move towards best whale\n",
    "                new_whale = np.array(best_whale) - A * np.abs(C * np.array(best_whale) - np.array(whale))\n",
    "        else:\n",
    "            distance_best = np.abs(np.array(best_whale) - np.array(whale))\n",
    "            new_whale = distance_best * np.exp(a * distance_best) * np.cos(2 * np.pi * np.random.rand())\n",
    "\n",
    "        # Bound check: Ensure the new whale is within bounds\n",
    "        new_whale = np.clip(new_whale, [self.bounds[param][0] for param in self.bounds],\n",
    "                            [self.bounds[param][1] for param in self.bounds])\n",
    "        \n",
    "        return new_whale\n",
    "\n",
    "    def optimize(self, smape_threshold=10.0):\n",
    "        \"\"\"Run the Whale Optimization Algorithm.\"\"\"\n",
    "        for t in tqdm(range(self.max_iter), desc=\"WOA Iterations\"):\n",
    "            a = 2 - t * (2 / self.max_iter)\n",
    "\n",
    "            for i in tqdm(range(self.n_whales), desc=f\"Whales (Iteration {t+1})\", leave=False):\n",
    "                whale = self.population[i]\n",
    "                hidden_size, num_layers, dropout, lr = whale\n",
    "\n",
    "                # print(f\"Iteration {t+1}, Whale {i+1}: hidden_size={int(hidden_size)}, \"\n",
    "                #       f\"num_layers={int(num_layers)}, dropout={dropout:.3f}, lr={lr:.6f}\")\n",
    "\n",
    "                fitness_val = self.fitness(whale)\n",
    "\n",
    "                if fitness_val < self.best_fitness:\n",
    "                    self.best_fitness = fitness_val\n",
    "                    self.best_whale = whale\n",
    "\n",
    "                    # Early stopping if precision threshold met\n",
    "                    if self.best_fitness <= smape_threshold:\n",
    "                        # print(f\"\\nStopping early at iteration {t+1} with SMAPE {self.best_fitness:.2f}\")\n",
    "                        return self.best_whale, self.best_fitness\n",
    "\n",
    "                self.population[i] = self.update_position(whale, a, self.best_whale)\n",
    "\n",
    "        return self.best_whale, self.best_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb30de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bounds for the hyperparameters\n",
    "bounds = {\n",
    "    'hidden_size': (32, 128),  # Increased hidden size range to 256\n",
    "    'num_layers': (1, 4),      # Increased num_layers range to 4\n",
    "    'dropout': (0.1, 0.5),     # Increased dropout range to 0.5\n",
    "    'lr': (5e-4, 1e-2)         # Increased learning rate range to 1e-2\n",
    "}\n",
    "\n",
    "# Create the WOA optimizer with the updated bounds\n",
    "woa = WOA(n_whales=5, max_iter=4, bounds=bounds)\n",
    "# Run optimization\n",
    "best_params, best_val_loss = woa.optimize()\n",
    "\n",
    "# Output the best found parameters and corresponding validation loss\n",
    "print(\"\\nBest configuration from WOA:\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d58eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Train model with best params from WOA\"\n",
    "import time  # Add this at the top of your script if not already imported\n",
    "# Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = X.shape[2]  # Number of features (should match the input size of the LSTM)\n",
    "\n",
    "model = LSTMForecast(\n",
    "    input_size=input_size,\n",
    "    hidden_size=int(best_params[0]),  # Ensure integer value for hidden_size\n",
    "    num_layers=int(best_params[1]),   # Ensure integer value for num_layers\n",
    "    dropout=best_params[2],\n",
    ").to(device)\n",
    "\n",
    "# Train the model with timing\n",
    "start_time = time.time()\n",
    "\n",
    "train_losses, val_losses = train_model(\n",
    "    model,\n",
    "    dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    epochs=100,\n",
    "    lr=best_params[3],\n",
    "    patience=15,       # stop if no val improvement\n",
    "    min_delta=1e-4     # must improve by at least this much\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTraining completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "plot_loss(train_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af78dd",
   "metadata": {},
   "source": [
    "# Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bed66e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_val_preds, y_val_true = [], []\n",
    "y_test_preds, y_test_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    if lookback >= 720:\n",
    "        # Lazy: batch-by-batch prediction\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            preds = model(xb)\n",
    "            y_val_preds.append(preds.cpu())\n",
    "            y_val_true.append(yb.cpu())\n",
    "\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            preds = model(xb)\n",
    "            y_test_preds.append(preds.cpu())\n",
    "            y_test_true.append(yb.cpu())\n",
    "\n",
    "        y_val_pred = torch.cat(y_val_preds).numpy()\n",
    "        y_val = torch.cat(y_val_true).numpy()\n",
    "        y_test_pred = torch.cat(y_test_preds).numpy()\n",
    "        y_test = torch.cat(y_test_true).numpy()\n",
    "\n",
    "    else:\n",
    "        # Eager mode\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        y_val_pred = model(X_val_tensor).cpu().numpy()\n",
    "        y_test_pred = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Reverse scaling\n",
    "y_val_pred_original = scaler_y.inverse_transform(y_val_pred)\n",
    "y_test_pred_original = scaler_y.inverse_transform(y_test_pred)\n",
    "\n",
    "y_val_original = scaler_y.inverse_transform(y_val.reshape(-1, 1))\n",
    "y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Flatten\n",
    "y_val_pred_original_flat = y_val_pred_original.flatten()\n",
    "y_test_pred_original_flat = y_test_pred_original.flatten()\n",
    "y_val_flat = y_val_original.flatten()\n",
    "y_test_flat = y_test_original.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf871081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datetime index from original DataFrame\n",
    "start_date = df.index.min()\n",
    "end_date = df.index.max()\n",
    "all_datetimes = pd.date_range(start=start_date, end=end_date, freq='h')\n",
    "N_total = len(all_datetimes)\n",
    "\n",
    "# Recalculate split indices\n",
    "train_size = int(0.7 * N_total)\n",
    "val_size = int(0.15 * N_total)\n",
    "val_start = train_size\n",
    "val_end = train_size + val_size\n",
    "test_start = val_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16fd2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datetime index for predictions\n",
    "val_index_expanded = pd.date_range(start=all_datetimes[val_start], periods=len(y_val_flat), freq='h')\n",
    "test_index_expanded = pd.date_range(start=all_datetimes[test_start], periods=len(y_test_flat), freq='h')\n",
    "\n",
    "# Create DataFrames for evaluation\n",
    "X_val_df = pd.DataFrame(index=val_index_expanded)\n",
    "X_test_df = pd.DataFrame(index=test_index_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f4e722ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_predictions(\n",
    "    y_true, y_pred, df_index,\n",
    "    start_time=\"2023-08-01 00:00:00\",\n",
    "    n_hours=500,\n",
    "    error_threshold=15\n",
    "):\n",
    "    # Convert inputs\n",
    "    start_time = pd.to_datetime(start_time)\n",
    "\n",
    "    # Build aligned DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'actual': y_true,\n",
    "        'predicted': y_pred\n",
    "    }, index=pd.to_datetime(df_index))\n",
    "\n",
    "    # Slice the time range\n",
    "    df_slice = df.loc[start_time : start_time + pd.Timedelta(hours=n_hours)]\n",
    "\n",
    "    # Calculate error\n",
    "    df_slice['error'] = abs(df_slice['actual'] - df_slice['predicted'])\n",
    "\n",
    "    # Identify high error regions\n",
    "    high_error_mask = df_slice['error'] > error_threshold\n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Actual values (Deep blue)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_slice.index, y=df_slice['actual'],\n",
    "        mode='lines', name='True',\n",
    "        line=dict(color='#1f77b4', width=2)\n",
    "    ))\n",
    "\n",
    "    # Predicted values (Soft green)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_slice.index, y=df_slice['predicted'],\n",
    "        mode='lines', name='Predicted',\n",
    "        line=dict(color='#2ca02c', width=2)\n",
    "    ))\n",
    "\n",
    "    # High-error markers (Bold red)\n",
    "    if high_error_mask.any():\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_slice.index[high_error_mask],\n",
    "            y=df_slice['predicted'][high_error_mask],\n",
    "            mode='markers',\n",
    "            marker=dict(size=6, color='#d62728', symbol='circle'),\n",
    "            name=f'Error > {error_threshold}',\n",
    "            text=[f\"Error: {e:.2f}\" for e in df_slice['error'][high_error_mask]],  # Hover text\n",
    "            hoverinfo='text+x+y',\n",
    "            showlegend=True\n",
    "        ))\n",
    "\n",
    "    # Layout styling\n",
    "    fig.update_layout(\n",
    "        title=f\"LSTM Forecast from {start_time.strftime('%Y-%m-%d %H:%M')} ({n_hours} hours)\",\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=\"Value\",\n",
    "        template=\"plotly_white\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "        height=500,\n",
    "        margin=dict(l=40, r=40, t=60, b=40)\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "51e0bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_predictions_matplotlib(y_true, y_pred, df_index, start_time, n_hours=500):\n",
    "    \"\"\"\n",
    "    y_true: true target values (numpy array or list)\n",
    "    y_pred: predicted values (numpy array or list)\n",
    "    df_index: datetime index corresponding to y_true\n",
    "    start_time: timestamp string from where to begin plotting\n",
    "    n_hours: total number of hours to plot (300 true + 200 predicted)\n",
    "    \"\"\"\n",
    "    # Ensure inputs are NumPy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Ensure df_index is a datetime index\n",
    "    if not isinstance(df_index, pd.DatetimeIndex):\n",
    "        df_index = pd.to_datetime(df_index)\n",
    "\n",
    "    # Find start index\n",
    "    start_idx = df_index.get_loc(pd.to_datetime(start_time))\n",
    "    end_idx = start_idx + n_hours\n",
    "\n",
    "    # Select the appropriate time window\n",
    "    time_window = df_index[start_idx:end_idx]\n",
    "    true_values_window = y_true[start_idx:end_idx]\n",
    "\n",
    "    # Separate segments\n",
    "    true_segment = true_values_window[:300]\n",
    "    true_segment_time = time_window[:300]\n",
    "\n",
    "    pred_segment_time = time_window[300:]\n",
    "    pred_segment_true = true_values_window[300:]\n",
    "    pred_segment_pred = y_pred[start_idx + 300:end_idx]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(true_segment_time, true_segment, label=\"True (past)\", color='#333333')\n",
    "    plt.plot(pred_segment_time, pred_segment_true, label=\"True (future)\", color='purple', linewidth=1)\n",
    "    plt.plot(pred_segment_time, pred_segment_pred, label=\"Predicted\", color='#B35C00', linestyle='dotted')\n",
    "\n",
    "    plt.title(\"Forecast from CNN-LSTM (90-Day Lookback): Observed vs Predicted Over 500 Hours\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price (USD)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab650b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_matplotlib(\n",
    "    y_true=y_val_flat,\n",
    "    y_pred=y_val_pred_original_flat,\n",
    "    df_index=X_val_df.index,\n",
    "    start_time=\"2024-04-01 00:00:00\",\n",
    "    n_hours=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c0d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate model ---\n",
    "results = evaluate_lstm(\n",
    "    y_val=y_val_flat,\n",
    "    y_val_pred=y_val_pred_original_flat,\n",
    "    y_test=y_test_flat,\n",
    "    y_test_pred=y_test_pred_original_flat,\n",
    "    X_val=X_val_df,\n",
    "    X_test=X_test_df)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17d202f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_datetime(model, df, timestamp, n_steps=48, target_col='Price (USD/MWh)', scaler_y=None):\n",
    "    \"\"\"\n",
    "    Predict 24-hour prices starting from a given timestamp.\n",
    "\n",
    "    Args:\n",
    "        model: Trained LSTMForecast model.\n",
    "        df: Preprocessed + scaled DataFrame (with DateTime index).\n",
    "        timestamp: Datetime string or pd.Timestamp (e.g. '2023-01-01 00:00').\n",
    "        n_steps: Number of past hours to use (default = 48).\n",
    "        target_col: Name of target column.\n",
    "        scaler_y: Scaler used for the target column.\n",
    "\n",
    "    Returns:\n",
    "        List of (datetime, predicted_price) tuples.\n",
    "    \"\"\"\n",
    "    if isinstance(timestamp, str):\n",
    "        timestamp = pd.Timestamp(timestamp)\n",
    "        \n",
    "    # Check if enough history is available\n",
    "    start_idx = df.index.get_loc(timestamp)\n",
    "    if start_idx < n_steps:\n",
    "        raise ValueError(\"Not enough history before this timestamp.\")\n",
    "\n",
    "    # Build the input sequence (excluding target columns)\n",
    "    seq_df = df.iloc[start_idx - n_steps:start_idx].drop(columns=[target_col, 'target_scaled'])\n",
    "    seq_input = seq_df.values  # shape: (n_steps, num_features)\n",
    "\n",
    "    # Predict the future prices\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        device = next(model.parameters()).device  # automatically gets model's device\n",
    "        x = torch.tensor(seq_input[np.newaxis, :, :], dtype=torch.float32).to(device)\n",
    "        y_pred = model(x).squeeze().cpu().numpy()\n",
    "\n",
    "    # Reverse scaling (if provided)\n",
    "    if scaler_y is not None:\n",
    "        y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1))  # Inverse transform predictions\n",
    "        y_pred_original_flat = y_pred_original.flatten()  # Flatten to 1D\n",
    "    else:\n",
    "        y_pred_original_flat = y_pred  # If no scaler, use raw predictions\n",
    "\n",
    "    # Build future timestamps\n",
    "    future_times = [timestamp + pd.Timedelta(hours=i) for i in range(24)]\n",
    "    \n",
    "    return list(zip(future_times, y_pred_original_flat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef25fe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict_from_datetime(model, df_scaled, '2024-01-01 00:00',scaler_y=scaler_y)\n",
    "\n",
    "for t, price in preds:\n",
    "    print(f\"{t}: ${price:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df8b64c",
   "metadata": {},
   "source": [
    "#  Post-Hoc Interpretability with Integrated Gradients (XAI)\n",
    "To understand how the LSTM model arrives at its predictions, we apply post-hoc interpretability methods—techniques used after model training to explain behavior without altering the model itself. One such method is Integrated Gradients, which attributes importance scores to input features based on their contribution to a specific prediction. This approach is particularly valuable for complex, black-box models like LSTMs, where internal mechanisms are not easily interpretable. By applying Integrated Gradients, we gain insights into both individual decisions (local interpretation) and broader model behavior (global interpretation), helping to build transparency, trust, and accountability in the forecasting process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc6c26b",
   "metadata": {},
   "source": [
    "## Local Interpretation\n",
    "The local interpretation using Integrated Gradients provides insight into which input features most influenced the model’s prediction for a specific timestamp. By attributing importance values to each feature in that one sample, we can understand the direction (positive or negative) and magnitude of their impact on the predicted price. This helps explain the model's reasoning at an individual decision level — for example, highlighting that high 7-day volatility or weekend timing pushed the forecast upward in that particular context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167851ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Temporarily switch to train mode for CuDNN backward compatibility\n",
    "model.train()\n",
    "\n",
    "all_attributions = []\n",
    "\n",
    "# Get feature names — adjust as needed to reflect correct columns\n",
    "# feature_names = features.columns.tolist()\n",
    "feature_names = df_scaled.drop(columns=['Price (USD/MWh)', 'target_scaled']).columns.tolist()\n",
    "\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "if lookback >= 720:\n",
    "    # Lazy case — use val_loader\n",
    "    for xb, _ in val_loader:\n",
    "        xb = xb.to(device).requires_grad_()  # shape: (batch, time, features)\n",
    "        for i in range(xb.size(0)):  # iterate through batch\n",
    "            input_tensor = xb[i:i+1]  # shape: (1, time, features)\n",
    "            attributions, _ = ig.attribute(input_tensor, target=0, return_convergence_delta=True)\n",
    "            summed = attributions.sum(dim=1).squeeze().detach().cpu().numpy()  # sum across time\n",
    "            all_attributions.append(summed)\n",
    "\n",
    "else:\n",
    "    # Non-lazy case — use X_val directly\n",
    "    for i in range(len(X_val)):\n",
    "        input_tensor = torch.tensor(X_val[i:i+1], dtype=torch.float32, requires_grad=True).to(device)\n",
    "        attributions, _ = ig.attribute(input_tensor, target=0, return_convergence_delta=True)\n",
    "        summed = attributions.sum(dim=1).squeeze().detach().cpu().numpy()\n",
    "        all_attributions.append(summed)\n",
    "\n",
    "model.eval()\n",
    "# Average across all samples\n",
    "avg_attr = np.mean(np.stack(all_attributions), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee1e07",
   "metadata": {},
   "source": [
    "The global interpretation aggregates feature attributions across many samples to show which inputs the model relies on most consistently. By averaging importance scores, it highlights the overall influence of each feature on predictions—revealing patterns such as persistent reliance on long-term volatility or cyclical time features. This offers a high-level understanding of the model’s behavior and helps validate that it aligns with domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cd698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot global feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(feature_names, avg_attr, color='darkorange')  # Set bar color\n",
    "plt.axhline(0, color='gray', linewidth=0.2, linestyle='-')  # Horizontal line at y=0\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Average Attribution\")\n",
    "plt.title(\"Global Feature Importance For CNN-LSTM\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
