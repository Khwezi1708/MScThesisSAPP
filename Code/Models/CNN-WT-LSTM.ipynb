{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f136a04",
   "metadata": {},
   "source": [
    "This file will run an LSTM model to predict the electricity prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b03c0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import plot_comparison, evaluate_lstm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "26a92140",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Data/zra_sgp_dam.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0032cb70",
   "metadata": {},
   "source": [
    "# Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "61e6a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cyclic(df, col, max_val):\n",
    "    \"\"\"\"\n",
    "    Time features like Hour, Month, day_of_week are cyclical, not linear. \n",
    "    Without encoding them properly, the model will misunderstand their relationships.\n",
    "    \"\"\"\n",
    "    df[col + '_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "    df[col + '_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "    return df\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Cyclic encode time features\n",
    "    df = encode_cyclic(df, 'Hour', 24)\n",
    "    df = encode_cyclic(df, 'Month', 12)\n",
    "    df = encode_cyclic(df, 'day_of_week', 7)\n",
    "    \n",
    "    # Drop unused or problematic columns\n",
    "    df = df.drop(columns=['Hour', 'Month', 'day_of_week'])  # Keep cyclic versions instead\n",
    "    \n",
    "    # Fill/clean if needed\n",
    "    df = df.fillna(method='ffill').dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "96285121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequence for LSTM adapted for WT\n",
    "def create_sequences_wt(data, target_col, lookback=48, forecast_horizon=24):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - lookback - forecast_horizon):\n",
    "        X_seq = data.iloc[i:i + lookback].values  # Use the lookback period for X\n",
    "        y_seq = data.iloc[i + lookback:i + lookback + forecast_horizon][target_col].values  # Multi-target for y\n",
    "        X.append(X_seq)\n",
    "        y.append(y_seq)\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "61e10133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7508771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WT_LSTMForecast(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2,\n",
    "                 forecast_horizon=24, output_dim=4):  # output_dim corresponds to number of coefficients you're forecasting\n",
    "        super().__init__()\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # [batch, seq_len, hidden_size]\n",
    "        lstm_out = lstm_out[:, -self.forecast_horizon:, :]  # Slicing to get the last 'forecast_horizon' time steps\n",
    "        out = self.fc(lstm_out)  # [batch, forecast_horizon, output_dim]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0e8b35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.log(torch.cosh(y_pred - y_true + 1e-12))  # added epsilon to prevent log(0)\n",
    "        return torch.mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9584fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "def train_model(model, dataloader, val_dataloader=None, epochs=10, lr=1e-3, patience=10, min_delta=1e-4):\n",
    "    device = next(model.parameters()).device\n",
    "    # criterion = LogCoshLoss()\n",
    "    criterion = nn.SmoothL1Loss()  # PyTorch’s implementation of Huber Loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = copy.deepcopy(model.state_dict())\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        loss_history.append(avg_loss)\n",
    "\n",
    "        if val_dataloader:\n",
    "            model.eval()\n",
    "            val_total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_dataloader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    pred = model(xb)\n",
    "                    loss = criterion(pred, yb)\n",
    "                    val_total_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = val_total_loss / len(val_dataloader)\n",
    "            val_loss_history.append(avg_val_loss)\n",
    "\n",
    "            if best_val_loss - avg_val_loss > min_delta:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"⏹️ Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if val_dataloader:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return loss_history, val_loss_history if val_dataloader else loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "556c9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_history):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(loss_history, label='Training loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f56ff6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_importance(importances, feature_names=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    names = list(feature_names) if feature_names is not None else [f\"Feature {i}\" for i in range(len(importances))]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(importances)), importances[indices])\n",
    "    plt.xticks(range(len(importances)), [names[i] for i in indices], rotation=45, ha='right')\n",
    "    plt.title(\"SHAP Feature Importance (Averaged)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f4f5eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_datetime(model, df, timestamp, n_steps=48, target_col='Price (USD/MWh)', scaler_y=None):\n",
    "    \"\"\"\n",
    "    Predict 24-hour prices starting from a given timestamp.\n",
    "\n",
    "    Args:\n",
    "        model: Trained LSTMForecast model.\n",
    "        df: Preprocessed + scaled DataFrame (with DateTime index).\n",
    "        timestamp: Datetime string or pd.Timestamp (e.g. '2023-01-01 00:00').\n",
    "        n_steps: Number of past hours to use (default = 48).\n",
    "        target_col: Name of target column.\n",
    "        scaler_y: Scaler used for the target column.\n",
    "\n",
    "    Returns:\n",
    "        List of (datetime, predicted_price) tuples.\n",
    "    \"\"\"\n",
    "    if isinstance(timestamp, str):\n",
    "        timestamp = pd.Timestamp(timestamp)\n",
    "        \n",
    "    # Check if enough history is available\n",
    "    start_idx = df.index.get_loc(timestamp)\n",
    "    if start_idx < n_steps:\n",
    "        raise ValueError(\"Not enough history before this timestamp.\")\n",
    "\n",
    "    # Build the input sequence (excluding target columns)\n",
    "    seq_df = df.iloc[start_idx - n_steps:start_idx].drop(columns=[target_col, 'target_scaled'])\n",
    "    seq_input = seq_df.values  # shape: (n_steps, num_features)\n",
    "\n",
    "    # Predict the future prices\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(seq_input[np.newaxis, :, :], dtype=torch.float32)\n",
    "        y_pred = model(x).squeeze().numpy()  # shape: (forecast_horizon, )\n",
    "\n",
    "    # Reverse scaling (if provided)\n",
    "    if scaler_y is not None:\n",
    "        y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1))  # Inverse transform predictions\n",
    "        y_pred_original_flat = y_pred_original.flatten()  # Flatten to 1D\n",
    "    else:\n",
    "        y_pred_original_flat = y_pred  # If no scaler, use raw predictions\n",
    "\n",
    "    # Build future timestamps\n",
    "    future_times = [timestamp + pd.Timedelta(hours=i) for i in range(24)]\n",
    "    \n",
    "    return list(zip(future_times, y_pred_original_flat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aed1bc",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a7ca40c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/krasmussen.11969746/ipykernel_2389902/4031471906.py:22: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='ffill').dropna()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Preprocess (make sure 'Date' becomes index)\n",
    "df_clean = preprocess(df)\n",
    "\n",
    "# Define target\n",
    "target_col = 'Price (USD/MWh)'\n",
    "features = df_clean.drop(columns=[target_col])\n",
    "target = df_clean[target_col]\n",
    "\n",
    "# Columns by type\n",
    "minmax_cols = ['Tati- normalised output', 'E_Grid (Mw)', 'Revenues (USD)', \n",
    "               'Flow_chavuma', 'Level_kariba', 'Flow_nana']\n",
    "standard_cols = ['Volatility_1 Day', 'Volatility_3 Days', 'Volatility_7 Days', 'Volatility_30 Days',\n",
    "                 'roc_49h', 'momentum_49h']\n",
    "no_scaling_cols = ['Hour_sin', 'Hour_cos', 'Month_sin', 'Month_cos',\n",
    "                   'day_of_week_sin', 'day_of_week_cos']\n",
    "\n",
    "# Initialize scalers\n",
    "minmax_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Copy clean DataFrame\n",
    "df_scaled = df_clean.copy()\n",
    "\n",
    "# Apply scalers to appropriate columns\n",
    "df_scaled[minmax_cols] = minmax_scaler.fit_transform(df_clean[minmax_cols])\n",
    "df_scaled[standard_cols] = standard_scaler.fit_transform(df_clean[standard_cols])\n",
    "\n",
    "# Target scaling (fit only on the column, keep shape)\n",
    "df_scaled[\"target_scaled\"] = scaler_y.fit_transform(df_clean[[target_col]])\n",
    "\n",
    "# Optionally retain unscaled target for reference\n",
    "df_scaled[target_col] = target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e038617",
   "metadata": {},
   "source": [
    "# CNN\n",
    "CNN-based feature extractor before creating sequences for your time series model.\n",
    "Use a 1D CNN to learn temporal features from the multivariate time series, and combine those with your original engineered features (like Fourier features, volatility, etc.) to build an enriched LSTM input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "665894bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels=16, kernel_size=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, out_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels, sequence_length)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        return x  # shape: (batch, out_channels, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0a0b6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_cnn_over_time(df_features, window=48, stride=1, device='cpu'):\n",
    "    \"\"\"\n",
    "    Apply CNN over rolling time windows of multivariate input.\n",
    "    Returns a DataFrame with CNN features per time step (centered).\n",
    "    \"\"\"\n",
    "    feature_cols = df_features.columns\n",
    "    num_features = len(feature_cols)\n",
    "    num_windows = (len(df_features) - window) // stride + 1\n",
    "\n",
    "    # Prepare input tensor\n",
    "    X_cnn = []\n",
    "    for i in range(0, len(df_features) - window + 1, stride):\n",
    "        window_data = df_features.iloc[i:i+window].values.T  # shape: (features, window)\n",
    "        X_cnn.append(window_data)\n",
    "\n",
    "    X_cnn = torch.tensor(np.stack(X_cnn), dtype=torch.float32).to(device)  # shape: (batch, features, window)\n",
    "\n",
    "    # CNN model\n",
    "    cnn = CNNFeatureExtractor(input_channels=num_features).to(device)\n",
    "    cnn.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features_out = cnn(X_cnn)  # shape: (batch, out_channels, window)\n",
    "    \n",
    "    # Collapse time dimension (e.g., take mean over time)\n",
    "    pooled = features_out.mean(dim=2).cpu().numpy()  # shape: (batch, out_channels)\n",
    "\n",
    "    # Align timestamps to center of each window\n",
    "    start_index = window // 2\n",
    "    end_index = start_index + len(pooled)\n",
    "    timestamps = df_features.index[start_index:end_index]\n",
    "\n",
    "    return pd.DataFrame(pooled, index=timestamps, columns=[f'cnn_feat_{i}' for i in range(pooled.shape[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a854f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_features_df = apply_cnn_over_time(\n",
    "    df_scaled[minmax_cols + standard_cols + no_scaling_cols],  # all features\n",
    "    window=48,\n",
    "    stride=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6dda9a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cnn_feat_0</th>\n",
       "      <th>cnn_feat_1</th>\n",
       "      <th>cnn_feat_2</th>\n",
       "      <th>cnn_feat_3</th>\n",
       "      <th>cnn_feat_4</th>\n",
       "      <th>cnn_feat_5</th>\n",
       "      <th>cnn_feat_6</th>\n",
       "      <th>cnn_feat_7</th>\n",
       "      <th>cnn_feat_8</th>\n",
       "      <th>cnn_feat_9</th>\n",
       "      <th>cnn_feat_10</th>\n",
       "      <th>cnn_feat_11</th>\n",
       "      <th>cnn_feat_12</th>\n",
       "      <th>cnn_feat_13</th>\n",
       "      <th>cnn_feat_14</th>\n",
       "      <th>cnn_feat_15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-02 00:00:00</th>\n",
       "      <td>0.899181</td>\n",
       "      <td>1.016788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.078132</td>\n",
       "      <td>0.691762</td>\n",
       "      <td>0.039071</td>\n",
       "      <td>0.898460</td>\n",
       "      <td>0.054143</td>\n",
       "      <td>0.004113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.143359</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>0.903649</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02 01:00:00</th>\n",
       "      <td>0.911076</td>\n",
       "      <td>1.019002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.074090</td>\n",
       "      <td>0.700485</td>\n",
       "      <td>0.047609</td>\n",
       "      <td>0.897236</td>\n",
       "      <td>0.054168</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.146567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.905322</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02 02:00:00</th>\n",
       "      <td>0.923928</td>\n",
       "      <td>1.023697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.069464</td>\n",
       "      <td>0.717447</td>\n",
       "      <td>0.058735</td>\n",
       "      <td>0.895968</td>\n",
       "      <td>0.053489</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.146429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900101</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02 03:00:00</th>\n",
       "      <td>0.924680</td>\n",
       "      <td>1.010086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.053806</td>\n",
       "      <td>0.723688</td>\n",
       "      <td>0.072753</td>\n",
       "      <td>0.895135</td>\n",
       "      <td>0.053487</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.145688</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.886276</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02 04:00:00</th>\n",
       "      <td>0.916191</td>\n",
       "      <td>0.990497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.029120</td>\n",
       "      <td>0.714114</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.888716</td>\n",
       "      <td>0.058187</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.132591</td>\n",
       "      <td>0.004276</td>\n",
       "      <td>0.871191</td>\n",
       "      <td>0.004731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-30 20:00:00</th>\n",
       "      <td>0.279465</td>\n",
       "      <td>0.118505</td>\n",
       "      <td>0.013468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045523</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.093746</td>\n",
       "      <td>0.206192</td>\n",
       "      <td>0.163989</td>\n",
       "      <td>0.042790</td>\n",
       "      <td>0.145560</td>\n",
       "      <td>0.202655</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.008080</td>\n",
       "      <td>0.198629</td>\n",
       "      <td>0.232783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-30 21:00:00</th>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.119049</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047998</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.093256</td>\n",
       "      <td>0.206091</td>\n",
       "      <td>0.161891</td>\n",
       "      <td>0.043855</td>\n",
       "      <td>0.148920</td>\n",
       "      <td>0.193161</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.006651</td>\n",
       "      <td>0.202532</td>\n",
       "      <td>0.230553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-30 22:00:00</th>\n",
       "      <td>0.279778</td>\n",
       "      <td>0.119296</td>\n",
       "      <td>0.015137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048555</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.090847</td>\n",
       "      <td>0.206909</td>\n",
       "      <td>0.158720</td>\n",
       "      <td>0.043473</td>\n",
       "      <td>0.153796</td>\n",
       "      <td>0.190089</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.005388</td>\n",
       "      <td>0.201732</td>\n",
       "      <td>0.230502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-30 23:00:00</th>\n",
       "      <td>0.278538</td>\n",
       "      <td>0.119011</td>\n",
       "      <td>0.013954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049396</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.090454</td>\n",
       "      <td>0.207875</td>\n",
       "      <td>0.154469</td>\n",
       "      <td>0.042960</td>\n",
       "      <td>0.157296</td>\n",
       "      <td>0.190606</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>0.198129</td>\n",
       "      <td>0.231303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 00:00:00</th>\n",
       "      <td>0.279062</td>\n",
       "      <td>0.117617</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048618</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.091760</td>\n",
       "      <td>0.208261</td>\n",
       "      <td>0.154688</td>\n",
       "      <td>0.042437</td>\n",
       "      <td>0.157030</td>\n",
       "      <td>0.189903</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>0.196081</td>\n",
       "      <td>0.232262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70081 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     cnn_feat_0  cnn_feat_1  cnn_feat_2  cnn_feat_3  \\\n",
       "Date                                                                  \n",
       "2017-01-02 00:00:00    0.899181    1.016788    0.000000    1.078132   \n",
       "2017-01-02 01:00:00    0.911076    1.019002    0.000000    1.074090   \n",
       "2017-01-02 02:00:00    0.923928    1.023697    0.000000    1.069464   \n",
       "2017-01-02 03:00:00    0.924680    1.010086    0.000000    1.053806   \n",
       "2017-01-02 04:00:00    0.916191    0.990497    0.000000    1.029120   \n",
       "...                         ...         ...         ...         ...   \n",
       "2024-12-30 20:00:00    0.279465    0.118505    0.013468    0.000000   \n",
       "2024-12-30 21:00:00    0.281000    0.119049    0.013636    0.000000   \n",
       "2024-12-30 22:00:00    0.279778    0.119296    0.015137    0.000000   \n",
       "2024-12-30 23:00:00    0.278538    0.119011    0.013954    0.000000   \n",
       "2024-12-31 00:00:00    0.279062    0.117617    0.013788    0.000000   \n",
       "\n",
       "                     cnn_feat_4  cnn_feat_5  cnn_feat_6  cnn_feat_7  \\\n",
       "Date                                                                  \n",
       "2017-01-02 00:00:00    0.691762    0.039071    0.898460    0.054143   \n",
       "2017-01-02 01:00:00    0.700485    0.047609    0.897236    0.054168   \n",
       "2017-01-02 02:00:00    0.717447    0.058735    0.895968    0.053489   \n",
       "2017-01-02 03:00:00    0.723688    0.072753    0.895135    0.053487   \n",
       "2017-01-02 04:00:00    0.714114    0.079366    0.888716    0.058187   \n",
       "...                         ...         ...         ...         ...   \n",
       "2024-12-30 20:00:00    0.045523    0.001325    0.093746    0.206192   \n",
       "2024-12-30 21:00:00    0.047998    0.001753    0.093256    0.206091   \n",
       "2024-12-30 22:00:00    0.048555    0.001721    0.090847    0.206909   \n",
       "2024-12-30 23:00:00    0.049396    0.002786    0.090454    0.207875   \n",
       "2024-12-31 00:00:00    0.048618    0.002791    0.091760    0.208261   \n",
       "\n",
       "                     cnn_feat_8  cnn_feat_9  cnn_feat_10  cnn_feat_11  \\\n",
       "Date                                                                    \n",
       "2017-01-02 00:00:00    0.004113    0.000000     0.217483     0.000000   \n",
       "2017-01-02 01:00:00    0.003476    0.000000     0.225802     0.000000   \n",
       "2017-01-02 02:00:00    0.001775    0.000000     0.234278     0.000000   \n",
       "2017-01-02 03:00:00    0.001731    0.000000     0.238243     0.000000   \n",
       "2017-01-02 04:00:00    0.001751    0.000000     0.242306     0.000000   \n",
       "...                         ...         ...          ...          ...   \n",
       "2024-12-30 20:00:00    0.163989    0.042790     0.145560     0.202655   \n",
       "2024-12-30 21:00:00    0.161891    0.043855     0.148920     0.193161   \n",
       "2024-12-30 22:00:00    0.158720    0.043473     0.153796     0.190089   \n",
       "2024-12-30 23:00:00    0.154469    0.042960     0.157296     0.190606   \n",
       "2024-12-31 00:00:00    0.154688    0.042437     0.157030     0.189903   \n",
       "\n",
       "                     cnn_feat_12  cnn_feat_13  cnn_feat_14  cnn_feat_15  \n",
       "Date                                                                     \n",
       "2017-01-02 00:00:00     1.143359     0.007405     0.903649     0.000000  \n",
       "2017-01-02 01:00:00     1.146567     0.000000     0.905322     0.000000  \n",
       "2017-01-02 02:00:00     1.146429     0.000000     0.900101     0.000000  \n",
       "2017-01-02 03:00:00     1.145688     0.001307     0.886276     0.000000  \n",
       "2017-01-02 04:00:00     1.132591     0.004276     0.871191     0.004731  \n",
       "...                          ...          ...          ...          ...  \n",
       "2024-12-30 20:00:00     0.000377     0.008080     0.198629     0.232783  \n",
       "2024-12-30 21:00:00     0.000377     0.006651     0.202532     0.230553  \n",
       "2024-12-30 22:00:00     0.000377     0.005388     0.201732     0.230502  \n",
       "2024-12-30 23:00:00     0.000420     0.005549     0.198129     0.231303  \n",
       "2024-12-31 00:00:00     0.000754     0.005549     0.196081     0.232262  \n",
       "\n",
       "[70081 rows x 16 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e315c4",
   "metadata": {},
   "source": [
    "# Applying WT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a3338055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align target with features\n",
    "cnn_features_df[\"target_scaled\"] = df_scaled[\"target_scaled\"].loc[cnn_features_df.index]\n",
    "df_model_input = cnn_features_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4d28b64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cnn_feat_0</th>\n",
       "      <th>cnn_feat_1</th>\n",
       "      <th>cnn_feat_2</th>\n",
       "      <th>cnn_feat_3</th>\n",
       "      <th>cnn_feat_4</th>\n",
       "      <th>cnn_feat_5</th>\n",
       "      <th>cnn_feat_6</th>\n",
       "      <th>cnn_feat_7</th>\n",
       "      <th>cnn_feat_8</th>\n",
       "      <th>cnn_feat_9</th>\n",
       "      <th>cnn_feat_10</th>\n",
       "      <th>cnn_feat_11</th>\n",
       "      <th>cnn_feat_12</th>\n",
       "      <th>cnn_feat_13</th>\n",
       "      <th>cnn_feat_14</th>\n",
       "      <th>cnn_feat_15</th>\n",
       "      <th>target_scaled</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-02 00:00:00</th>\n",
       "      <td>0.899181</td>\n",
       "      <td>1.016788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.078132</td>\n",
       "      <td>0.691762</td>\n",
       "      <td>0.039071</td>\n",
       "      <td>0.898460</td>\n",
       "      <td>0.054143</td>\n",
       "      <td>0.004113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.143359</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>0.903649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02 01:00:00</th>\n",
       "      <td>0.911076</td>\n",
       "      <td>1.019002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.074090</td>\n",
       "      <td>0.700485</td>\n",
       "      <td>0.047609</td>\n",
       "      <td>0.897236</td>\n",
       "      <td>0.054168</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.146567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.905322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02 02:00:00</th>\n",
       "      <td>0.923928</td>\n",
       "      <td>1.023697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.069464</td>\n",
       "      <td>0.717447</td>\n",
       "      <td>0.058735</td>\n",
       "      <td>0.895968</td>\n",
       "      <td>0.053489</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.146429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02 03:00:00</th>\n",
       "      <td>0.924680</td>\n",
       "      <td>1.010086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.053806</td>\n",
       "      <td>0.723688</td>\n",
       "      <td>0.072753</td>\n",
       "      <td>0.895135</td>\n",
       "      <td>0.053487</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.145688</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.886276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02 04:00:00</th>\n",
       "      <td>0.916191</td>\n",
       "      <td>0.990497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.029120</td>\n",
       "      <td>0.714114</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.888716</td>\n",
       "      <td>0.058187</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.132591</td>\n",
       "      <td>0.004276</td>\n",
       "      <td>0.871191</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>0.079225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-30 20:00:00</th>\n",
       "      <td>0.279465</td>\n",
       "      <td>0.118505</td>\n",
       "      <td>0.013468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045523</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.093746</td>\n",
       "      <td>0.206192</td>\n",
       "      <td>0.163989</td>\n",
       "      <td>0.042790</td>\n",
       "      <td>0.145560</td>\n",
       "      <td>0.202655</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.008080</td>\n",
       "      <td>0.198629</td>\n",
       "      <td>0.232783</td>\n",
       "      <td>0.867875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-30 21:00:00</th>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.119049</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047998</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.093256</td>\n",
       "      <td>0.206091</td>\n",
       "      <td>0.161891</td>\n",
       "      <td>0.043855</td>\n",
       "      <td>0.148920</td>\n",
       "      <td>0.193161</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.006651</td>\n",
       "      <td>0.202532</td>\n",
       "      <td>0.230553</td>\n",
       "      <td>0.621950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-30 22:00:00</th>\n",
       "      <td>0.279778</td>\n",
       "      <td>0.119296</td>\n",
       "      <td>0.015137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048555</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.090847</td>\n",
       "      <td>0.206909</td>\n",
       "      <td>0.158720</td>\n",
       "      <td>0.043473</td>\n",
       "      <td>0.153796</td>\n",
       "      <td>0.190089</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.005388</td>\n",
       "      <td>0.201732</td>\n",
       "      <td>0.230502</td>\n",
       "      <td>0.488125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-30 23:00:00</th>\n",
       "      <td>0.278538</td>\n",
       "      <td>0.119011</td>\n",
       "      <td>0.013954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049396</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.090454</td>\n",
       "      <td>0.207875</td>\n",
       "      <td>0.154469</td>\n",
       "      <td>0.042960</td>\n",
       "      <td>0.157296</td>\n",
       "      <td>0.190606</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>0.198129</td>\n",
       "      <td>0.231303</td>\n",
       "      <td>0.488125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 00:00:00</th>\n",
       "      <td>0.279062</td>\n",
       "      <td>0.117617</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048618</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.091760</td>\n",
       "      <td>0.208261</td>\n",
       "      <td>0.154688</td>\n",
       "      <td>0.042437</td>\n",
       "      <td>0.157030</td>\n",
       "      <td>0.189903</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>0.196081</td>\n",
       "      <td>0.232262</td>\n",
       "      <td>0.249875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70081 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     cnn_feat_0  cnn_feat_1  cnn_feat_2  cnn_feat_3  \\\n",
       "Date                                                                  \n",
       "2017-01-02 00:00:00    0.899181    1.016788    0.000000    1.078132   \n",
       "2017-01-02 01:00:00    0.911076    1.019002    0.000000    1.074090   \n",
       "2017-01-02 02:00:00    0.923928    1.023697    0.000000    1.069464   \n",
       "2017-01-02 03:00:00    0.924680    1.010086    0.000000    1.053806   \n",
       "2017-01-02 04:00:00    0.916191    0.990497    0.000000    1.029120   \n",
       "...                         ...         ...         ...         ...   \n",
       "2024-12-30 20:00:00    0.279465    0.118505    0.013468    0.000000   \n",
       "2024-12-30 21:00:00    0.281000    0.119049    0.013636    0.000000   \n",
       "2024-12-30 22:00:00    0.279778    0.119296    0.015137    0.000000   \n",
       "2024-12-30 23:00:00    0.278538    0.119011    0.013954    0.000000   \n",
       "2024-12-31 00:00:00    0.279062    0.117617    0.013788    0.000000   \n",
       "\n",
       "                     cnn_feat_4  cnn_feat_5  cnn_feat_6  cnn_feat_7  \\\n",
       "Date                                                                  \n",
       "2017-01-02 00:00:00    0.691762    0.039071    0.898460    0.054143   \n",
       "2017-01-02 01:00:00    0.700485    0.047609    0.897236    0.054168   \n",
       "2017-01-02 02:00:00    0.717447    0.058735    0.895968    0.053489   \n",
       "2017-01-02 03:00:00    0.723688    0.072753    0.895135    0.053487   \n",
       "2017-01-02 04:00:00    0.714114    0.079366    0.888716    0.058187   \n",
       "...                         ...         ...         ...         ...   \n",
       "2024-12-30 20:00:00    0.045523    0.001325    0.093746    0.206192   \n",
       "2024-12-30 21:00:00    0.047998    0.001753    0.093256    0.206091   \n",
       "2024-12-30 22:00:00    0.048555    0.001721    0.090847    0.206909   \n",
       "2024-12-30 23:00:00    0.049396    0.002786    0.090454    0.207875   \n",
       "2024-12-31 00:00:00    0.048618    0.002791    0.091760    0.208261   \n",
       "\n",
       "                     cnn_feat_8  cnn_feat_9  cnn_feat_10  cnn_feat_11  \\\n",
       "Date                                                                    \n",
       "2017-01-02 00:00:00    0.004113    0.000000     0.217483     0.000000   \n",
       "2017-01-02 01:00:00    0.003476    0.000000     0.225802     0.000000   \n",
       "2017-01-02 02:00:00    0.001775    0.000000     0.234278     0.000000   \n",
       "2017-01-02 03:00:00    0.001731    0.000000     0.238243     0.000000   \n",
       "2017-01-02 04:00:00    0.001751    0.000000     0.242306     0.000000   \n",
       "...                         ...         ...          ...          ...   \n",
       "2024-12-30 20:00:00    0.163989    0.042790     0.145560     0.202655   \n",
       "2024-12-30 21:00:00    0.161891    0.043855     0.148920     0.193161   \n",
       "2024-12-30 22:00:00    0.158720    0.043473     0.153796     0.190089   \n",
       "2024-12-30 23:00:00    0.154469    0.042960     0.157296     0.190606   \n",
       "2024-12-31 00:00:00    0.154688    0.042437     0.157030     0.189903   \n",
       "\n",
       "                     cnn_feat_12  cnn_feat_13  cnn_feat_14  cnn_feat_15  \\\n",
       "Date                                                                      \n",
       "2017-01-02 00:00:00     1.143359     0.007405     0.903649     0.000000   \n",
       "2017-01-02 01:00:00     1.146567     0.000000     0.905322     0.000000   \n",
       "2017-01-02 02:00:00     1.146429     0.000000     0.900101     0.000000   \n",
       "2017-01-02 03:00:00     1.145688     0.001307     0.886276     0.000000   \n",
       "2017-01-02 04:00:00     1.132591     0.004276     0.871191     0.004731   \n",
       "...                          ...          ...          ...          ...   \n",
       "2024-12-30 20:00:00     0.000377     0.008080     0.198629     0.232783   \n",
       "2024-12-30 21:00:00     0.000377     0.006651     0.202532     0.230553   \n",
       "2024-12-30 22:00:00     0.000377     0.005388     0.201732     0.230502   \n",
       "2024-12-30 23:00:00     0.000420     0.005549     0.198129     0.231303   \n",
       "2024-12-31 00:00:00     0.000754     0.005549     0.196081     0.232262   \n",
       "\n",
       "                     target_scaled  \n",
       "Date                                \n",
       "2017-01-02 00:00:00       0.079975  \n",
       "2017-01-02 01:00:00       0.079525  \n",
       "2017-01-02 02:00:00       0.079525  \n",
       "2017-01-02 03:00:00       0.079325  \n",
       "2017-01-02 04:00:00       0.079225  \n",
       "...                            ...  \n",
       "2024-12-30 20:00:00       0.867875  \n",
       "2024-12-30 21:00:00       0.621950  \n",
       "2024-12-30 22:00:00       0.488125  \n",
       "2024-12-30 23:00:00       0.488125  \n",
       "2024-12-31 00:00:00       0.249875  \n",
       "\n",
       "[70081 rows x 17 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c54fbd",
   "metadata": {},
   "source": [
    "Applying WT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ba13c08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM input shape: (8761, 37)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tati- normalised output</th>\n",
       "      <th>E_Grid (Mw)</th>\n",
       "      <th>Revenues (USD)</th>\n",
       "      <th>Volatility_1 Day</th>\n",
       "      <th>Volatility_3 Days</th>\n",
       "      <th>Volatility_7 Days</th>\n",
       "      <th>Volatility_30 Days</th>\n",
       "      <th>Flow_chavuma</th>\n",
       "      <th>Level_kariba</th>\n",
       "      <th>Flow_nana</th>\n",
       "      <th>...</th>\n",
       "      <th>Hour_sin</th>\n",
       "      <th>Hour_cos</th>\n",
       "      <th>Month_sin</th>\n",
       "      <th>Month_cos</th>\n",
       "      <th>day_of_week_sin</th>\n",
       "      <th>day_of_week_cos</th>\n",
       "      <th>cA_3</th>\n",
       "      <th>cD_3</th>\n",
       "      <th>cD_2</th>\n",
       "      <th>cD_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.121125</td>\n",
       "      <td>0.121125</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>-1.603109</td>\n",
       "      <td>-1.903265</td>\n",
       "      <td>-1.982205</td>\n",
       "      <td>-2.07914</td>\n",
       "      <td>0.305269</td>\n",
       "      <td>0.180531</td>\n",
       "      <td>0.065688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657975</td>\n",
       "      <td>0.504882</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.225399</td>\n",
       "      <td>-0.000292</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.000318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.596008</td>\n",
       "      <td>0.596008</td>\n",
       "      <td>0.052648</td>\n",
       "      <td>-1.603109</td>\n",
       "      <td>-1.903265</td>\n",
       "      <td>-1.982205</td>\n",
       "      <td>-2.07914</td>\n",
       "      <td>0.305269</td>\n",
       "      <td>0.180531</td>\n",
       "      <td>0.065688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108253</td>\n",
       "      <td>-0.822265</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.243642</td>\n",
       "      <td>-0.001564</td>\n",
       "      <td>0.002213</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.030277</td>\n",
       "      <td>0.030277</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>-1.592212</td>\n",
       "      <td>-1.903265</td>\n",
       "      <td>-1.982205</td>\n",
       "      <td>-2.07914</td>\n",
       "      <td>0.305269</td>\n",
       "      <td>0.180531</td>\n",
       "      <td>0.065688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.766229</td>\n",
       "      <td>0.317382</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.294634</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>-0.004225</td>\n",
       "      <td>-0.002369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.160270</td>\n",
       "      <td>0.160270</td>\n",
       "      <td>0.013787</td>\n",
       "      <td>-1.516439</td>\n",
       "      <td>-1.903265</td>\n",
       "      <td>-1.982205</td>\n",
       "      <td>-2.07914</td>\n",
       "      <td>0.313858</td>\n",
       "      <td>0.179646</td>\n",
       "      <td>0.066620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657975</td>\n",
       "      <td>0.504882</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.224082</td>\n",
       "      <td>-0.067511</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>-0.005215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.981445</td>\n",
       "      <td>0.981445</td>\n",
       "      <td>0.090532</td>\n",
       "      <td>-1.518805</td>\n",
       "      <td>-1.903265</td>\n",
       "      <td>-1.982205</td>\n",
       "      <td>-2.07914</td>\n",
       "      <td>0.313858</td>\n",
       "      <td>0.179646</td>\n",
       "      <td>0.066620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108253</td>\n",
       "      <td>-0.822265</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.459310</td>\n",
       "      <td>0.035364</td>\n",
       "      <td>-0.040188</td>\n",
       "      <td>-0.001450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tati- normalised output  E_Grid (Mw)  Revenues (USD)  Volatility_1 Day  \\\n",
       "0                 0.121125     0.121125        0.009766         -1.603109   \n",
       "1                 0.596008     0.596008        0.052648         -1.603109   \n",
       "2                 0.030277     0.030277        0.002924         -1.592212   \n",
       "3                 0.160270     0.160270        0.013787         -1.516439   \n",
       "4                 0.981445     0.981445        0.090532         -1.518805   \n",
       "\n",
       "   Volatility_3 Days  Volatility_7 Days  Volatility_30 Days  Flow_chavuma  \\\n",
       "0          -1.903265          -1.982205            -2.07914      0.305269   \n",
       "1          -1.903265          -1.982205            -2.07914      0.305269   \n",
       "2          -1.903265          -1.982205            -2.07914      0.305269   \n",
       "3          -1.903265          -1.982205            -2.07914      0.313858   \n",
       "4          -1.903265          -1.982205            -2.07914      0.313858   \n",
       "\n",
       "   Level_kariba  Flow_nana  ...  Hour_sin  Hour_cos  Month_sin  Month_cos  \\\n",
       "0      0.180531   0.065688  ...  0.657975  0.504882        0.5   0.866025   \n",
       "1      0.180531   0.065688  ...  0.108253 -0.822265        0.5   0.866025   \n",
       "2      0.180531   0.065688  ... -0.766229  0.317382        0.5   0.866025   \n",
       "3      0.179646   0.066620  ...  0.657975  0.504882        0.5   0.866025   \n",
       "4      0.179646   0.066620  ...  0.108253 -0.822265        0.5   0.866025   \n",
       "\n",
       "   day_of_week_sin  day_of_week_cos      cA_3      cD_3      cD_2      cD_1  \n",
       "0        -0.781831          0.62349  0.225399 -0.000292  0.000325  0.000318  \n",
       "1        -0.781831          0.62349  0.243642 -0.001564  0.002213  0.000141  \n",
       "2        -0.781831          0.62349  0.294634  0.019481 -0.004225 -0.002369  \n",
       "3         0.000000          1.00000  0.224082 -0.067511  0.001888 -0.005215  \n",
       "4         0.000000          1.00000  0.459310  0.035364 -0.040188 -0.001450  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pywt\n",
    "\n",
    "# ---- Step 1: Extract normalized target as numpy array ----\n",
    "target_array = df_model_input[\"target_scaled\"].values\n",
    "\n",
    "# ---- Step 2: Apply single-level Wavelet Decomposition ----\n",
    "# --- Parameters ---\n",
    "wavelet = 'db1'       # You can experiment with other wavelets\n",
    "level = 3             # Number of decomposition levels\n",
    "\n",
    "# --- Apply Multi-Level DWT ---\n",
    "coeffs = pywt.wavedec(target_array, wavelet=wavelet, level=level)\n",
    "\n",
    "# Denoising (optional)\n",
    "threshold = np.std(coeffs[-1]) * np.sqrt(2 * np.log(len(target_array)))\n",
    "denoised_coeffs = [coeffs[0]]  # Approximation stays unchanged\n",
    "for cd in coeffs[1:]:\n",
    "    denoised_cd = pywt.threshold(cd, threshold, mode='soft')\n",
    "    denoised_coeffs.append(denoised_cd)\n",
    "\n",
    "# --- Reconstruct signal (optional step, useful for visualization) ---\n",
    "signal_denoised = pywt.waverec(denoised_coeffs, wavelet=wavelet)\n",
    "\n",
    "# --- Format features ---\n",
    "def pad_or_truncate(arr, target_len):\n",
    "    if len(arr) < target_len:\n",
    "        return np.pad(arr, (0, target_len - len(arr)), mode='constant')\n",
    "    else:\n",
    "        return arr[:target_len]\n",
    "\n",
    "min_len = min(len(c) for c in coeffs)\n",
    "features_wt = np.column_stack([pad_or_truncate(c, min_len) for c in coeffs])\n",
    "feature_names = [f'cA_{level}'] + [f'cD_{i}' for i in range(level, 0, -1)]\n",
    "df_wt = pd.DataFrame(features_wt, columns=feature_names)\n",
    "\n",
    "# Downsample explanatory features to match min_len\n",
    "features_only = df_scaled.drop(columns=[target_col, \"target_scaled\"]).values\n",
    "factor = len(features_only) // min_len\n",
    "features_downsampled = features_only[:factor * min_len].reshape(min_len, factor, features_only.shape[1]).mean(axis=1)\n",
    "\n",
    "# Combine downsampled features with WT features\n",
    "df_lstm_input = pd.DataFrame(\n",
    "    np.hstack([features_downsampled, df_wt.values]),\n",
    "    columns=[*df_scaled.drop(columns=[target_col, \"target_scaled\"]).columns, *feature_names]\n",
    ")\n",
    "\n",
    "# Inspect final input\n",
    "print(\"LSTM input shape:\", df_lstm_input.shape)\n",
    "df_lstm_input.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d876fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 24\n",
    "lookback = 24 * 30 # 60 days of lookback\n",
    "\n",
    "# Create sequences for each WT coefficient\n",
    "X, y_cA = create_sequences_wt(df_lstm_input, target_col='cA_3', lookback=lookback, forecast_horizon=forecast_horizon)\n",
    "_, y_d3 = create_sequences_wt(df_lstm_input, target_col='cD_3', lookback=lookback, forecast_horizon=forecast_horizon)\n",
    "_, y_d2 = create_sequences_wt(df_lstm_input, target_col='cD_2', lookback=lookback, forecast_horizon=forecast_horizon)\n",
    "_, y_d1 = create_sequences_wt(df_lstm_input, target_col='cD_1', lookback=lookback, forecast_horizon=forecast_horizon)\n",
    "\n",
    "# Stack all coefficients into final target\n",
    "y = np.stack([y_cA, y_d3, y_d2, y_d1], axis=-1)  # shape: (samples, forecast_horizon, 4)# Train-validation-test split (70-15-15 split)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Create datasets\n",
    "train_ds = PriceDataset(X_train, y_train)\n",
    "val_ds = PriceDataset(X_val, y_val)\n",
    "test_ds = PriceDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=False, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d531474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import uniform\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming LSTMForecast and train_ds are defined elsewhere\n",
    "# Also assuming val_loader is defined somewhere\n",
    "\n",
    "input_size = X.shape[2]  # Number of features (should match the input size of the LSTM)\n",
    "\n",
    "class WOA:\n",
    "    def __init__(self, n_whales, max_iter, bounds):\n",
    "        self.n_whales = n_whales\n",
    "        self.max_iter = max_iter\n",
    "        self.bounds = bounds\n",
    "        self.population = self.init_population()\n",
    "        self.best_whale = None\n",
    "        self.best_fitness = float('inf')\n",
    "\n",
    "    def init_population(self):\n",
    "        \"\"\"Initialize whale population randomly within the specified bounds.\"\"\"\n",
    "        population = []\n",
    "        for _ in range(self.n_whales):\n",
    "            whale = [uniform(*self.bounds[param]) for param in self.bounds]\n",
    "            population.append(whale)\n",
    "        return population\n",
    "\n",
    "    def smape(self, y_true, y_pred):\n",
    "        \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "        smape = np.mean(np.abs(y_pred - y_true) / denominator) * 100\n",
    "        return smape\n",
    "    \n",
    "    def evaluate_model(self, model, val_loader, device):\n",
    "        \"\"\"Evaluate the model on the validation set.\"\"\"\n",
    "        model.to(device)  # Ensure model is on the correct device (GPU or CPU)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        with torch.no_grad():  # No gradient calculation needed during evaluation\n",
    "            for xb, yb in val_loader:  # Iterate over validation data\n",
    "                xb, yb = xb.to(device), yb.to(device)  # Move data to the correct device\n",
    "                output = model(xb)  # Model's predictions\n",
    "\n",
    "                y_true.append(yb.cpu().numpy())  # Store actual values\n",
    "                y_pred.append(output.cpu().numpy())  # Store predicted values\n",
    "\n",
    "        # Flatten the lists into 1D arrays for easy SMAPE calculation\n",
    "        y_true = np.concatenate(y_true, axis=0)\n",
    "        y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "        return y_true, y_pred\n",
    "\n",
    "    def fitness(self, whale):\n",
    "        \"\"\"Calculate fitness (SMAPE) of the current whale configuration.\"\"\"\n",
    "        hidden_size, num_layers, dropout, lr = whale\n",
    "\n",
    "        N_subset = 8760 - lookback\n",
    "        subset_indices = list(range(N_subset))\n",
    "        subset_train_ds = torch.utils.data.Subset(train_ds, subset_indices)\n",
    "        subset_train_loader = DataLoader(subset_train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        model = WT_LSTMForecast(\n",
    "            input_size=input_size,\n",
    "            hidden_size=int(hidden_size),\n",
    "            num_layers=int(num_layers),\n",
    "            dropout=dropout,\n",
    "            output_dim=4\n",
    "        ).to(device)  # Move the model to the correct device\n",
    "\n",
    "        # Get actuals and predictions from validation\n",
    "        y_true, y_pred = self.evaluate_model(model, val_loader, device)\n",
    "\n",
    "        return self.smape(y_true, y_pred)\n",
    "\n",
    "\n",
    "    def update_position(self, whale, a, best_whale):\n",
    "        \"\"\"Update the position of the whale based on the hunting behavior.\"\"\"\n",
    "        A = 2 * a * np.random.rand() - a  # Randomization for exploration\n",
    "        C = 2 * np.random.rand()  # Another randomization factor for exploration\n",
    "        p = np.random.rand()  # Probability for exploitation or exploration\n",
    "\n",
    "        if p < 0.5:\n",
    "            if np.abs(A) >= 1:\n",
    "                # Exploration: Random movement\n",
    "                rand_whale = self.population[np.random.randint(self.n_whales)]\n",
    "                new_whale = np.array(rand_whale) - A * np.abs(C * np.array(rand_whale) - np.array(whale))\n",
    "            else:\n",
    "                # Exploitation: Move towards best whale\n",
    "                new_whale = np.array(best_whale) - A * np.abs(C * np.array(best_whale) - np.array(whale))\n",
    "        else:\n",
    "            distance_best = np.abs(np.array(best_whale) - np.array(whale))\n",
    "            new_whale = distance_best * np.exp(a * distance_best) * np.cos(2 * np.pi * np.random.rand())\n",
    "\n",
    "        # Bound check: Ensure the new whale is within bounds\n",
    "        new_whale = np.clip(new_whale, [self.bounds[param][0] for param in self.bounds],\n",
    "                            [self.bounds[param][1] for param in self.bounds])\n",
    "        \n",
    "        return new_whale\n",
    "\n",
    "    def optimize(self, smape_threshold=10.0):\n",
    "        \"\"\"Run the Whale Optimization Algorithm.\"\"\"\n",
    "        for t in tqdm(range(self.max_iter), desc=\"WOA Iterations\"):\n",
    "            a = 2 - t * (2 / self.max_iter)\n",
    "\n",
    "            for i in tqdm(range(self.n_whales), desc=f\"Whales (Iteration {t+1})\", leave=False):\n",
    "                whale = self.population[i]\n",
    "                hidden_size, num_layers, dropout, lr = whale\n",
    "\n",
    "                # print(f\"Iteration {t+1}, Whale {i+1}: hidden_size={int(hidden_size)}, \"\n",
    "                #       f\"num_layers={int(num_layers)}, dropout={dropout:.3f}, lr={lr:.6f}\")\n",
    "\n",
    "                fitness_val = self.fitness(whale)\n",
    "\n",
    "                if fitness_val < self.best_fitness:\n",
    "                    self.best_fitness = fitness_val\n",
    "                    self.best_whale = whale\n",
    "\n",
    "                    # Early stopping if precision threshold met\n",
    "                    if self.best_fitness <= smape_threshold:\n",
    "                        # print(f\"\\nStopping early at iteration {t+1} with SMAPE {self.best_fitness:.2f}\")\n",
    "                        return self.best_whale, self.best_fitness\n",
    "\n",
    "                self.population[i] = self.update_position(whale, a, self.best_whale)\n",
    "\n",
    "        return self.best_whale, self.best_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "50bb30de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WOA Iterations: 100%|██████████| 50/50 [03:21<00:00,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best configuration from WOA:\n",
      "Best Params: [3.2e+01 1.0e+00 1.0e-01 5.0e-04]\n",
      "Best Validation Loss: 135.89743041992188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the bounds for the hyperparameters\n",
    "bounds = {\n",
    "    'hidden_size': (32, 256),  # Increased hidden size range to 256\n",
    "    'num_layers': (1, 4),      # Increased num_layers range to 4\n",
    "    'dropout': (0.1, 0.5),     # Increased dropout range to 0.5\n",
    "    'lr': (5e-4, 1e-2)         # Increased learning rate range to 1e-2\n",
    "}\n",
    "\n",
    "# Create the WOA optimizer with the updated bounds\n",
    "woa = WOA(n_whales=10, max_iter=50, bounds=bounds)\n",
    "# Run optimization\n",
    "best_params, best_val_loss = woa.optimize()\n",
    "\n",
    "# Output the best found parameters and corresponding validation loss\n",
    "print(\"\\nBest configuration from WOA:\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ba8d58eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping at epoch 21\n",
      "\n",
      "Training completed in 119.71 seconds.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"Train model with best params from WOA\"\n",
    "import time  # Add this at the top of your script if not already imported\n",
    "# Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = X.shape[2]  # Number of features (should match the input size of the LSTM)\n",
    "\n",
    "model = WT_LSTMForecast(\n",
    "    input_size=input_size,\n",
    "    hidden_size=int(256),  # Ensure integer value for hidden_size\n",
    "    num_layers=int(2),   # Ensure integer value for num_layers\n",
    "    dropout=best_params[2]\n",
    ").to(device)\n",
    "\n",
    "# Train the model with timing\n",
    "start_time = time.time()\n",
    "\n",
    "train_losses, val_losses = train_model(\n",
    "    model,\n",
    "    dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    epochs=100,\n",
    "    lr=best_params[3],\n",
    "    patience=15,       # stop if no val improvement\n",
    "    min_delta=1e-4     # must improve by at least this much\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTraining completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "plot_loss(train_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af78dd",
   "metadata": {},
   "source": [
    "# Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d0c0a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch model to eval mode\n",
    "model.eval()\n",
    "\n",
    "def get_predictions(model, dataloader):\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in dataloader:\n",
    "            xb = xb.to(device)\n",
    "            preds = model(xb)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "    return np.concatenate(all_preds, axis=0)\n",
    "\n",
    "# Get predictions from validation and test loaders\n",
    "val_pred_np = get_predictions(model, val_loader)  # <-- Now this is defined\n",
    "test_pred_np = get_predictions(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4e178d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_from_coeffs(y_pred, wavelet='db1'):\n",
    "    \"\"\"\n",
    "    Reconstruct time series from predicted DWT coefficients.\n",
    "    y_pred: numpy array of shape (samples, forecast_horizon, 4)\n",
    "    Returns:\n",
    "        reconstructed: numpy array of shape (samples, forecast_horizon)\n",
    "    \"\"\"\n",
    "    reconstructed = []\n",
    "    for sample in y_pred:\n",
    "        # sample: (forecast_horizon, 4)\n",
    "        sample_recon = []\n",
    "        for t in range(sample.shape[0]):\n",
    "            cA3, cD3, cD2, cD1 = sample[t]\n",
    "\n",
    "            # Use pywt.waverec to reconstruct from multi-level coeffs\n",
    "            coeffs = [np.array([cA3]), np.array([cD3]), np.array([cD2]), np.array([cD1])]\n",
    "            signal = pywt.waverec(coeffs, wavelet)\n",
    "\n",
    "            # Take only the first value (or mean) to align with time step t\n",
    "            sample_recon.append(signal[0])  # or signal.mean(), etc.\n",
    "\n",
    "        reconstructed.append(sample_recon)\n",
    "\n",
    "    return np.array(reconstructed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5c242339",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = reconstruct_from_coeffs(val_pred_np)\n",
    "reconstructed = scaler_y.inverse_transform(reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c5b3e7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1203, 24, 4)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "52de5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_val = reconstruct_from_coeffs(y_val)\n",
    "true_test = reconstruct_from_coeffs(y_test)\n",
    "\n",
    "true_val = scaler_y.inverse_transform(true_val)\n",
    "true_test = scaler_y.inverse_transform(true_test)\n",
    "\n",
    "y_val_flat = true_val.flatten()\n",
    "y_test_flat = true_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "80336881",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_val = reconstruct_from_coeffs(val_pred_np)\n",
    "reconstructed_val = scaler_y.inverse_transform(reconstructed_val)\n",
    "\n",
    "reconstructed_test = reconstruct_from_coeffs(test_pred_np)\n",
    "reconstructed_test = scaler_y.inverse_transform(reconstructed_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "44c8abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_val_flat = reconstructed_val.flatten()\n",
    "recon_test_flat = reconstructed_test.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cf871081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datetime index from original DataFrame\n",
    "start_date = df.index.min()\n",
    "end_date = df.index.max()\n",
    "all_datetimes = pd.date_range(start=start_date, end=end_date, freq='h')\n",
    "N_total = len(all_datetimes)\n",
    "\n",
    "# Recalculate split indices\n",
    "train_size = int(0.7 * N_total)\n",
    "val_size = int(0.15 * N_total)\n",
    "val_start = train_size\n",
    "val_end = train_size + val_size\n",
    "test_start = val_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "16fd2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datetime index for predictions\n",
    "val_index_expanded = pd.date_range(start=all_datetimes[val_start], periods=len(y_val_flat), freq='h')\n",
    "test_index_expanded = pd.date_range(start=all_datetimes[test_start], periods=len(y_test_flat), freq='h')\n",
    "\n",
    "# Create DataFrames for evaluation\n",
    "X_val_df = pd.DataFrame(index=val_index_expanded)\n",
    "X_test_df = pd.DataFrame(index=test_index_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f4e722ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_predictions(\n",
    "    y_true, y_pred, df_index,\n",
    "    start_time=\"2023-08-01 00:00:00\",\n",
    "    n_hours=500,\n",
    "    error_threshold=15\n",
    "):\n",
    "    # Convert inputs\n",
    "    start_time = pd.to_datetime(start_time)\n",
    "\n",
    "    # Build aligned DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'actual': y_true,\n",
    "        'predicted': y_pred\n",
    "    }, index=pd.to_datetime(df_index))\n",
    "\n",
    "    # Slice the time range\n",
    "    df_slice = df.loc[start_time : start_time + pd.Timedelta(hours=n_hours)]\n",
    "\n",
    "    # Calculate error\n",
    "    df_slice['error'] = abs(df_slice['actual'] - df_slice['predicted'])\n",
    "\n",
    "    # Identify high error regions\n",
    "    high_error_mask = df_slice['error'] > error_threshold\n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Actual values (Deep blue)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_slice.index, y=df_slice['actual'],\n",
    "        mode='lines', name='True',\n",
    "        line=dict(color='#1f77b4', width=2)\n",
    "    ))\n",
    "\n",
    "    # Predicted values (Soft green)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_slice.index, y=df_slice['predicted'],\n",
    "        mode='lines', name='Predicted',\n",
    "        line=dict(color='#2ca02c', width=2)\n",
    "    ))\n",
    "\n",
    "    # High-error markers (Bold red)\n",
    "    if high_error_mask.any():\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_slice.index[high_error_mask],\n",
    "            y=df_slice['predicted'][high_error_mask],\n",
    "            mode='markers',\n",
    "            marker=dict(size=6, color='#d62728', symbol='circle'),\n",
    "            name=f'Error > {error_threshold}',\n",
    "            text=[f\"Error: {e:.2f}\" for e in df_slice['error'][high_error_mask]],  # Hover text\n",
    "            hoverinfo='text+x+y',\n",
    "            showlegend=True\n",
    "        ))\n",
    "\n",
    "    # Layout styling\n",
    "    fig.update_layout(\n",
    "        title=f\"LSTM Forecast from {start_time.strftime('%Y-%m-%d %H:%M')} ({n_hours} hours)\",\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=\"Value\",\n",
    "        template=\"plotly_white\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "        height=500,\n",
    "        margin=dict(l=40, r=40, t=60, b=40)\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "61bd73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_predictions(\n",
    "#     y_true=y_val_flat,\n",
    "#     y_pred=recon_val_flat,  # Use the reconstructed, inverse-scaled prediction\n",
    "#     df_index=X_val_df.index,  # Assuming you have this aligned with y_val\n",
    "#     start_time=\"2023-08-01 00:00:00\",\n",
    "#     n_hours=500\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "56c0d771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Metric        MAE        DAE        RMSE        R2  \\\n",
      "0  Validation Set  68.788321  37.506116   85.937986 -0.019076   \n",
      "1        Test Set  74.072805  63.383894  104.959034 -0.295580   \n",
      "\n",
      "   Lower Predictions (%)  \n",
      "0              43.897201  \n",
      "1              66.067470  \n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate model with reconstructed predictions ---\n",
    "results = evaluate_lstm(\n",
    "    y_val=y_val_flat,\n",
    "    y_val_pred=recon_val_flat,       # updated\n",
    "    y_test=y_test_flat,\n",
    "    y_test_pred=recon_test_flat,     # updated\n",
    "    X_val=X_val_df,\n",
    "    X_test=X_test_df\n",
    ")\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "36114238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 68.788\n",
      "RMSE: 85.938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "mae = mean_absolute_error(y_val_flat, recon_val_flat)\n",
    "rmse = root_mean_squared_error(y_val_flat, recon_val_flat)\n",
    "\n",
    "print(f\"MAE: {mae:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "17d202f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_datetime(model, df, timestamp, n_steps=48, target_col='Price (USD/MWh)', scaler_y=None):\n",
    "    \"\"\"\n",
    "    Predict 24-hour prices starting from a given timestamp.\n",
    "\n",
    "    Args:\n",
    "        model: Trained LSTMForecast model.\n",
    "        df: Preprocessed + scaled DataFrame (with DateTime index).\n",
    "        timestamp: Datetime string or pd.Timestamp (e.g. '2023-01-01 00:00').\n",
    "        n_steps: Number of past hours to use (default = 48).\n",
    "        target_col: Name of target column.\n",
    "        scaler_y: Scaler used for the target column.\n",
    "\n",
    "    Returns:\n",
    "        List of (datetime, predicted_price) tuples.\n",
    "    \"\"\"\n",
    "    if isinstance(timestamp, str):\n",
    "        timestamp = pd.Timestamp(timestamp)\n",
    "        \n",
    "    # Check if enough history is available\n",
    "    start_idx = df.index.get_loc(timestamp)\n",
    "    if start_idx < n_steps:\n",
    "        raise ValueError(\"Not enough history before this timestamp.\")\n",
    "\n",
    "    # Build the input sequence (excluding target columns)\n",
    "    seq_df = df.iloc[start_idx - n_steps:start_idx].drop(columns=[target_col, 'target_scaled'])\n",
    "    seq_input = seq_df.values  # shape: (n_steps, num_features)\n",
    "\n",
    "    # Predict the future prices\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(seq_input[np.newaxis, :, :], dtype=torch.float32)\n",
    "        y_pred = model(x).squeeze().numpy()  # shape: (forecast_horizon, )\n",
    "\n",
    "    # Reverse scaling (if provided)\n",
    "    if scaler_y is not None:\n",
    "        y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1))  # Inverse transform predictions\n",
    "        y_pred_original_flat = y_pred_original.flatten()  # Flatten to 1D\n",
    "    else:\n",
    "        y_pred_original_flat = y_pred  # If no scaler, use raw predictions\n",
    "\n",
    "    # Build future timestamps\n",
    "    future_times = [timestamp + pd.Timedelta(hours=i) for i in range(24)]\n",
    "    \n",
    "    return list(zip(future_times, y_pred_original_flat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ef25fe44",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 37, got 33",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m preds = \u001b[43mpredict_from_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2024-01-01 00:00\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mscaler_y\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaler_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t, price \u001b[38;5;129;01min\u001b[39;00m preds:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprice\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[131]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mpredict_from_datetime\u001b[39m\u001b[34m(model, df, timestamp, n_steps, target_col, scaler_y)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     31\u001b[39m     x = torch.tensor(seq_input[np.newaxis, :, :], dtype=torch.float32)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     y_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.squeeze().numpy()  \u001b[38;5;66;03m# shape: (forecast_horizon, )\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Reverse scaling (if provided)\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scaler_y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mWT_LSTMForecast.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     lstm_out, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, seq_len, hidden_size]\u001b[39;00m\n\u001b[32m     21\u001b[39m     lstm_out = lstm_out[:, -\u001b[38;5;28mself\u001b[39m.forecast_horizon:, :]  \u001b[38;5;66;03m# Slicing to get the last 'forecast_horizon' time steps\u001b[39;00m\n\u001b[32m     22\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.fc(lstm_out)  \u001b[38;5;66;03m# [batch, forecast_horizon, output_dim]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1101\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1093\u001b[39m     c_zeros = torch.zeros(\n\u001b[32m   1094\u001b[39m         \u001b[38;5;28mself\u001b[39m.num_layers * num_directions,\n\u001b[32m   1095\u001b[39m         max_batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1098\u001b[39m         device=\u001b[38;5;28minput\u001b[39m.device,\n\u001b[32m   1099\u001b[39m     )\n\u001b[32m   1100\u001b[39m     hx = (h_zeros, c_zeros)\n\u001b[32m-> \u001b[39m\u001b[32m1101\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1002\u001b[39m, in \u001b[36mLSTM.check_forward_args\u001b[39m\u001b[34m(self, input, hidden, batch_sizes)\u001b[39m\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_forward_args\u001b[39m(\n\u001b[32m    997\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    998\u001b[39m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[32m    999\u001b[39m     hidden: \u001b[38;5;28mtuple\u001b[39m[Tensor, Tensor],  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m   1000\u001b[39m     batch_sizes: Optional[Tensor],\n\u001b[32m   1001\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_hidden_size(\n\u001b[32m   1004\u001b[39m         hidden[\u001b[32m0\u001b[39m],\n\u001b[32m   1005\u001b[39m         \u001b[38;5;28mself\u001b[39m.get_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[32m   1006\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1007\u001b[39m     )\n\u001b[32m   1008\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_hidden_size(\n\u001b[32m   1009\u001b[39m         hidden[\u001b[32m1\u001b[39m],\n\u001b[32m   1010\u001b[39m         \u001b[38;5;28mself\u001b[39m.get_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[32m   1011\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1012\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/nn/modules/rnn.py:314\u001b[39m, in \u001b[36mRNNBase.check_input\u001b[39m\u001b[34m(self, input, batch_sizes)\u001b[39m\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    311\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.dim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    312\u001b[39m     )\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.input_size != \u001b[38;5;28minput\u001b[39m.size(-\u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    315\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.input_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size(-\u001b[32m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    316\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: input.size(-1) must be equal to input_size. Expected 37, got 33"
     ]
    }
   ],
   "source": [
    "preds = predict_from_datetime(model, df_scaled, '2024-01-01 00:00',scaler_y=scaler_y)\n",
    "\n",
    "for t, price in preds:\n",
    "    print(f\"{t}: ${price:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984f399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd979aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6a6b6a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8df8b64c",
   "metadata": {},
   "source": [
    "#  Post-Hoc Interpretability with Integrated Gradients (XAI)\n",
    "To understand how the LSTM model arrives at its predictions, we apply post-hoc interpretability methods—techniques used after model training to explain behavior without altering the model itself. One such method is Integrated Gradients, which attributes importance scores to input features based on their contribution to a specific prediction. This approach is particularly valuable for complex, black-box models like LSTMs, where internal mechanisms are not easily interpretable. By applying Integrated Gradients, we gain insights into both individual decisions (local interpretation) and broader model behavior (global interpretation), helping to build transparency, trust, and accountability in the forecasting process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc6c26b",
   "metadata": {},
   "source": [
    "## Local Interpretation\n",
    "The local interpretation using Integrated Gradients provides insight into which input features most influenced the model’s prediction for a specific timestamp. By attributing importance values to each feature in that one sample, we can understand the direction (positive or negative) and magnitude of their impact on the predicted price. This helps explain the model's reasoning at an individual decision level — for example, highlighting that high 7-day volatility or weekend timing pushed the forecast upward in that particular context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167851ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Extract feature names from your DataFrame\n",
    "feature_names = features.columns.tolist()  # must match order in X_val\n",
    "\n",
    "# Select a sample\n",
    "sample_idx = 0 # sample_idx is simply the index of a single sample (row) in your validation dataset (X_val)\n",
    "input_tensor = torch.tensor(X_val[sample_idx:sample_idx+1], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Initialize Integrated Gradients\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "# Compute attributions for the first output (e.g., first forecasted hour)\n",
    "attributions, delta = ig.attribute(\n",
    "    input_tensor, target=0, return_convergence_delta=True\n",
    ")\n",
    "\n",
    "# Sum attributions across the time dimension\n",
    "attributions_sum = attributions.sum(dim=1).squeeze().detach().numpy()\n",
    "\n",
    "# Plot feature attributions with proper labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(feature_names, attributions_sum)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Attribution\")\n",
    "plt.title(\"Local Feature Attributions using Integrated Gradients\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee1e07",
   "metadata": {},
   "source": [
    "The global interpretation aggregates feature attributions across many samples to show which inputs the model relies on most consistently. By averaging importance scores, it highlights the overall influence of each feature on predictions—revealing patterns such as persistent reliance on long-term volatility or cyclical time features. This offers a high-level understanding of the model’s behavior and helps validate that it aligns with domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cd698",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attributions = []\n",
    "model.eval()\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    input_tensor = torch.tensor(X_val[i:i+1], dtype=torch.float32, requires_grad=True)\n",
    "    attributions, _ = ig.attribute(input_tensor, target=0, return_convergence_delta=True)\n",
    "    summed = attributions.sum(dim=1).squeeze().detach().numpy()\n",
    "    all_attributions.append(summed)\n",
    "\n",
    "# Now average across all samples\n",
    "avg_attr = np.mean(np.stack(all_attributions), axis=0)\n",
    "\n",
    "# Plot global feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(feature_names, avg_attr)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Average Attribution\")\n",
    "plt.title(\"Global Feature Importance via Integrated Gradients\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
